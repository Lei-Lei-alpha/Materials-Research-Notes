{"/Materials-Research-Notes/Chap1-Coding/Python-Code%20snipet/": {
    "title": "Python - useful snippets for materials science",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap1-Coding/Python-Code%20snipet/",
    "body": "Fast distance calculation With periodic boundary condition stackoverflow.com These are 8 different solutions I’ve timed, some of my own and some posted in response to my question, that use 4 broad approaches: spatial cdist spatial KDtree Sklearn BallTree Kernel approach This is the code with the 8 test routines: import numpy as np from scipy import spatial from sklearn.neighbors import BallTree n=500 # size of 2D box f=200./(n*n) # first number is rough number of target cells... np.random.seed(1) # to make reproducable a=np.random.uniform(size=(n,n)) i=np.argwhere(a&gt;-1) # all points, we want to know distance to nearest point j=np.argwhere(a&gt;1.0-f) # set of locations to find distance to. # long array of 3x3 j points: for xoff in [0,n,-n]: for yoff in [0,-n,n]: if xoff==0 and yoff==0: j9=j.copy() else: jo=j.copy() jo[:,0]+=xoff jo[:,1]+=yoff j9=np.vstack((j9,jo)) global maxdist maxdist=10 overlap=5.2 kernel_size=int(np.sqrt(overlap*n**2/j.shape[0])/2) print(\"no points\",len(j)) # repear cdist over each member of 3x3 block def dist_v1(i,j): dist=[] # 3x3 search required for periodic boundaries. for xoff in [-n,0,n]: for yoff in [-n,0,n]: jo=j.copy() jo[:,0]+=xoff jo[:,1]+=yoff dist.append(np.amin(spatial.distance.cdist(i,jo,metric='euclidean'),1)) dist=np.amin(np.stack(dist),0).reshape([n,n]) #dmask=np.where(dist&lt;=maxdist,1,0) return(dist) # same as v1, but taking one amin function at the end def dist_v2(i,j): dist=[] # 3x3 search required for periodic boundaries. for xoff in [-n,0,n]: for yoff in [-n,0,n]: jo=j.copy() jo[:,0]+=xoff jo[:,1]+=yoff dist.append(spatial.distance.cdist(i,jo,metric='euclidean')) dist=np.amin(np.dstack(dist),(1,2)).reshape([n,n]) #dmask=np.where(dist&lt;=maxdist,1,0) return(dist) # using a KDTree query ball points, looping over j9 points as in online example def dist_v3(n,j): x,y=np.mgrid[0:n,0:n] points=np.c_[x.ravel(), y.ravel()] tree=spatial.KDTree(points) mask=np.zeros([n,n]) for results in tree.query_ball_point((j), 2.1): mask[points[results][:,0],points[results][:,1]]=1 return(mask) # using ckdtree query on the j9 long array def dist_v4(i,j): tree=spatial.cKDTree(j) dist,minid=tree.query(i) return(dist.reshape([n,n])) # back to using Cdist, but on the long j9 3x3 array, rather than on each element separately def dist_v5(i,j): # 3x3 search required for periodic boundaries. dist=np.amin(spatial.distance.cdist(i,j,metric='euclidean'),1) #dmask=np.where(dist&lt;=maxdist,1,0) return(dist) def dist_v6(i,j): tree = BallTree(j,leaf_size=5,metric='euclidean') dist = tree.query(i, k=1, return_distance=True) mindist = dist[0].reshape(n,n) return(mindist) def sq_distance(x1, y1, x2, y2, n): # computes the pairwise squared distance between 2 sets of points (with periodicity) # x1, y1 : coordinates of the first set of points (source) # x2, y2 : same dx = np.abs((np.subtract.outer(x1, x2) + n//2)%(n) - n//2) dy = np.abs((np.subtract.outer(y1, y2) + n//2)%(n) - n//2) d = (dx*dx + dy*dy) return d def apply_kernel1(sources, sqdist, kern_size, n, mask): ker_i, ker_j = np.meshgrid(np.arange(-kern_size, kern_size+1), np.arange(-kern_size, kern_size+1), indexing=\"ij\") kernel = np.add.outer(np.arange(-kern_size, kern_size+1)**2, np.arange(-kern_size, kern_size+1)**2) mask_kernel = kernel &gt; kern_size**2 for pi, pj in sources: ind_i = (pi+ker_i)%n ind_j = (pj+ker_j)%n sqdist[ind_i,ind_j] = np.minimum(kernel, sqdist[ind_i,ind_j]) mask[ind_i,ind_j] *= mask_kernel def apply_kernel2(sources, sqdist, kern_size, n, mask): ker_i = np.arange(-kern_size, kern_size+1).reshape((2*kern_size+1,1)) ker_j = np.arange(-kern_size, kern_size+1).reshape((1,2*kern_size+1)) kernel = np.add.outer(np.arange(-kern_size, kern_size+1)**2, np.arange(-kern_size, kern_size+1)**2) mask_kernel = kernel &gt; kern_size**2 for pi, pj in sources: imin = pi-kern_size jmin = pj-kern_size imax = pi+kern_size+1 jmax = pj+kern_size+1 if imax &lt; n and jmax &lt; n and imin &gt;=0 and jmin &gt;=0: # we are inside sqdist[imin:imax,jmin:jmax] = np.minimum(kernel, sqdist[imin:imax,jmin:jmax]) mask[imin:imax,jmin:jmax] *= mask_kernel elif imax &lt; n and imin &gt;=0: ind_j = (pj+ker_j.ravel())%n sqdist[imin:imax,ind_j] = np.minimum(kernel, sqdist[imin:imax,ind_j]) mask[imin:imax,ind_j] *= mask_kernel elif jmax &lt; n and jmin &gt;=0: ind_i = (pi+ker_i.ravel())%n sqdist[ind_i,jmin:jmax] = np.minimum(kernel, sqdist[ind_i,jmin:jmax]) mask[ind_i,jmin:jmax] *= mask_kernel else : ind_i = (pi+ker_i)%n ind_j = (pj+ker_j)%n sqdist[ind_i,ind_j] = np.minimum(kernel, sqdist[ind_i,ind_j]) mask[ind_i,ind_j] *= mask_kernel def dist_v7(sources, n, kernel_size, method): sources = np.asfortranarray(sources) #for memory contiguity kernel_size = min(kernel_size, n//2) kernel_size = max(kernel_size, 1) sqdist = np.full((n,n), 10*n**2, dtype=np.int32) #preallocate with a huge distance (&gt;max**2) mask = np.ones((n,n), dtype=bool) #which points have not been reached? #main code if (method == 1): apply_kernel1(sources, sqdist, kernel_size, n, mask) else: apply_kernel2(sources, sqdist, kernel_size, n, mask) #remaining points rem_i, rem_j = np.nonzero(mask) if len(rem_i) &gt; 0: sq_d = sq_distance(sources[:,0], sources[:,1], rem_i, rem_j, n).min(axis=0) sqdist[rem_i, rem_j] = sq_d return np.sqrt(sqdist) from timeit import timeit nl=10 print (\"-----------------------\") print (\"Timings for \",nl,\"loops\") print (\"-----------------------\") print(\"1. cdist looped amin:\",timeit(lambda: dist_v1(i,j),number=nl)) print(\"2. cdist single amin:\",timeit(lambda: dist_v2(i,j),number=nl)) print(\"3. KDtree ball pt:\", timeit(lambda: dist_v3(n,j9),number=nl)) print(\"4. KDtree query:\",timeit(lambda: dist_v4(i,j9),number=nl)) print(\"5. cdist long list:\",timeit(lambda: dist_v5(i,j9),number=nl)) print(\"6. ball tree:\",timeit(lambda: dist_v6(i,j9),number=nl)) print(\"7. kernel orig:\", timeit(lambda: dist_v7(j, n, kernel_size,1), number=nl)) print(\"8. kernel optimised:\", timeit(lambda: dist_v7(j, n, kernel_size,2), number=nl)) The output (timing in seconds) on my linux 12 core desktop (with 48GB RAM) for n=350 and 63 points: no points 63 ----------------------- Timings for 10 loops ----------------------- 1. cdist looped amin: 3.2488364999881014 2. cdist single amin: 6.494611179979984 3. KDtree ball pt: 5.180531410995172 4. KDtree query: 0.9377906009904109 5. cdist long list: 3.906166430999292 6. ball tree: 3.3540162370190956 7. kernel orig: 0.7813036740117241 8. kernel optimised: 0.17046571199898608 and for n=500 and npts=176: no points 176 ----------------------- Timings for 10 loops ----------------------- 1. cdist looped amin: 16.787221198988846 2. cdist single amin: 40.97849371898337 3. KDtree ball pt: 9.926229109987617 4. KDtree query: 0.8417396580043714 5. cdist long list: 14.345821461000014 6. ball tree: 1.8792325239919592 7. kernel orig: 1.0807358759921044 8. kernel optimised: 0.5650744160229806 So in summary I reached the following conclusions: avoid cdist if you have quite a large problem If your problem is not too computational-time constrained I would recommend the “KDtree query” approach as it is just 2 lines without the periodic boundaries and a few more with periodic boundary to set up the j9 array For maximum performance (e.g. a long integration of a model where this is required each time step as is my case) then the Kernal solution is now by far the fastest. stackoverflow.com You should write your distance() function in a way that you can vectorise the loop over the 5711 points. The following implementation accepts an array of points as either the x0 or x1 parameter: def distance(x0, x1, dimensions): delta = numpy.abs(x0 - x1) delta = numpy.where(delta &gt; 0.5 * dimensions, delta - dimensions, delta) return numpy.sqrt((delta ** 2).sum(axis=-1)) Example: &gt;&gt;&gt; dimensions = numpy.array([3.0, 4.0, 5.0]) &gt;&gt;&gt; points = numpy.array([[2.7, 1.5, 4.3], [1.2, 0.3, 4.2]]) &gt;&gt;&gt; distance(points, [1.5, 2.0, 2.5], dimensions) array([ 2.22036033, 2.42280829]) The result is the array of distances between the points passed as second parameter to distance() and each point in points."
  },"/Materials-Research-Notes/Chap1-Coding/Python-data%20model/": {
    "title": "Python - data structures",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap1-Coding/Python-data%20model/",
    "body": "Data model Special methods Also known as magic methods, usually written with leading and tailing double under scores. Implement when we want our objects to support and interact with fundamental language constructs such as: Collections Attribute access Iteration"
  },"/Materials-Research-Notes/Chap1-Coding/Python-tricks/": {
    "title": "Python practical tricks",
    "keywords": "Python notes",
    "url": "/Materials-Research-Notes/Chap1-Coding/Python-tricks/",
    "body": "Processing Files Copy files In Python, you can copy the files using shutil module os module subprocess module import os import shutil import subprocess Copying files using shutil module shutil.copyfile signature shutil.copyfile(src_file, dest_file, *, follow_symlinks=True) # example shutil.copyfile('source.txt', 'destination.txt') shutil.copy signature shutil.copy(src_file, dest_file, *, follow_symlinks=True) # example shutil.copy('source.txt', 'destination.txt') shutil.copy2 signature shutil.copy2(src_file, dest_file, *, follow_symlinks=True) # example shutil.copy2('source.txt', 'destination.txt') shutil.copyfileobj signature shutil.copyfileobj(src_file_object, dest_file_object[, length]) # example file_src = 'source.txt' f_src = open(file_src, 'rb') file_dest = 'destination.txt' f_dest = open(file_dest, 'wb') shutil.copyfileobj(f_src, f_dest) Clarification on shutil module Function Copies metadata Copies permissions Uses file object Destination may be directory shutil.copy No Yes No Yes shutil.copyfile No No No No shutil.copy2 Yes Yes No Yes shutil.copyfileobj No No Yes No 9 Note that even the shutil.copy2() function cannot copy all file metadata. – wovano Mar 11, 2022 at 10:34 Copying files using os module os.popen signature os.popen(cmd[, mode[, bufsize]]) # example # In Unix/Linux os.popen('cp source.txt destination.txt') # In Windows os.popen('copy source.txt destination.txt') os.system signature os.system(command) # In Linux/Unix os.system('cp source.txt destination.txt') # In Windows os.system('copy source.txt destination.txt') Copying files using subprocess module subprocess.call signature subprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False) # example (WARNING: setting `shell=True` might be a security-risk) # In Linux/Unix status = subprocess.call('cp source.txt destination.txt', shell=True) # In Windows status = subprocess.call('copy source.txt destination.txt', shell=True) subprocess.check_output signature subprocess.check_output(args, *, stdin=None, stderr=None, shell=False, universal_newlines=False) # example (WARNING: setting `shell=True` might be a security-risk) # In Linux/Unix status = subprocess.check_output('cp source.txt destination.txt', shell=True) # In Windows status = subprocess.check_output('copy source.txt destination.txt', shell=True) JSON Load json file import json with open('site_occ.json') as file: parsed_json = json.load(file) print(parsed_json) List, tuple and dict Remove all occurences of an element from a list Remove all occurrences of a value from a list? Functional approach: Python 3.x &gt;&gt;&gt; x = [1,2,3,2,2,2,3,4] &gt;&gt;&gt; list(filter((2).__ne__, x)) [1, 3, 3, 4] Sort list of lists Source stackoverflow.com &gt;&gt;&gt; k = [[1, 2], [4], [5, 6, 2], [1, 2], [3], [4]] &gt;&gt;&gt; import itertools &gt;&gt;&gt; k.sort() &gt;&gt;&gt; list(k for k,_ in itertools.groupby(k)) [[1, 2], [3], [4], [5, 6, 2]] itertools often offers the fastest and most powerful solutions to this kind of problems, and is well worth getting intimately familiar with! Edit: as I mention in a comment, normal optimization efforts are focused on large inputs (the big-O approach) because it’s so much easier that it offers good returns on efforts. But sometimes (essentially for “tragically crucial bottlenecks” in deep inner loops of code that’s pushing the boundaries of performance limits) one may need to go into much more detail, providing probability distributions, deciding which performance measures to optimize (maybe the upper bound or the 90th centile is more important than an average or median, depending on one’s apps), performing possibly-heuristic checks at the start to pick different algorithms depending on input data characteristics, and so forth. Careful measurements of “point” performance (code A vs code B for a specific input) are a part of this extremely costly process, and standard library module timeit helps here. However, it’s easier to use it at a shell prompt. For example, here’s a short module to showcase the general approach for this problem, save it as nodup.py: import itertools k = [[1, 2], [4], [5, 6, 2], [1, 2], [3], [4]] def doset(k, map=map, list=list, set=set, tuple=tuple): return map(list, set(map(tuple, k))) def dosort(k, sorted=sorted, xrange=xrange, len=len): ks = sorted(k) return [ks[i] for i in xrange(len(ks)) if i == 0 or ks[i] != ks[i-1]] def dogroupby(k, sorted=sorted, groupby=itertools.groupby, list=list): ks = sorted(k) return [i for i, _ in itertools.groupby(ks)] def donewk(k): newk = [] for i in k: if i not in newk: newk.append(i) return newk # sanity check that all functions compute the same result and don't alter k if __name__ == '__main__': savek = list(k) for f in doset, dosort, dogroupby, donewk: resk = f(k) assert k == savek print '%10s %s' % (f.__name__, sorted(resk)) Note the sanity check (performed when you just do python nodup.py) and the basic hoisting technique (make constant global names local to each function for speed) to put things on equal footing. Now we can run checks on the tiny example list: $ python -mtimeit -s'import nodup' 'nodup.doset(nodup.k)' 100000 loops, best of 3: 11.7 usec per loop $ python -mtimeit -s'import nodup' 'nodup.dosort(nodup.k)' 100000 loops, best of 3: 9.68 usec per loop $ python -mtimeit -s'import nodup' 'nodup.dogroupby(nodup.k)' 100000 loops, best of 3: 8.74 usec per loop $ python -mtimeit -s'import nodup' 'nodup.donewk(nodup.k)' 100000 loops, best of 3: 4.44 usec per loop confirming that the quadratic approach has small-enough constants to make it attractive for tiny lists with few duplicated values. With a short list without duplicates: $ python -mtimeit -s'import nodup' 'nodup.donewk([[i] for i in range(12)])' 10000 loops, best of 3: 25.4 usec per loop $ python -mtimeit -s'import nodup' 'nodup.dogroupby([[i] for i in range(12)])' 10000 loops, best of 3: 23.7 usec per loop $ python -mtimeit -s'import nodup' 'nodup.doset([[i] for i in range(12)])' 10000 loops, best of 3: 31.3 usec per loop $ python -mtimeit -s'import nodup' 'nodup.dosort([[i] for i in range(12)])' 10000 loops, best of 3: 25 usec per loop the quadratic approach isn’t bad, but the sort and groupby ones are better. Etc, etc. If (as the obsession with performance suggests) this operation is at a core inner loop of your pushing-the-boundaries application, it’s worth trying the same set of tests on other representative input samples, possibly detecting some simple measure that could heuristically let you pick one or the other approach (but the measure must be fast, of course). It’s also well worth considering keeping a different representation for k – why does it have to be a list of lists rather than a set of tuples in the first place? If the duplicate removal task is frequent, and profiling shows it to be the program’s performance bottleneck, keeping a set of tuples all the time and getting a list of lists from it only if and where needed, might be faster overall, for example. Sort list of dicts Sort by values Source: stackoverflow.com The sorted() function takes a key= parameter newlist = sorted(list_to_be_sorted, key=lambda d: d['name']) Alternatively, you can use operator.itemgetter instead of defining the function yourself from operator import itemgetter newlist = sorted(list_to_be_sorted, key=itemgetter('name')) For completeness, add reverse=True to sort in descending order newlist = sorted(list_to_be_sorted, key=itemgetter('name'), reverse=True) Sort by key names Source: stackoverflow.com Just use sorted using a list like [key1 in dict, key2 in dict, ...] as the key to sort by. Remember to reverse the result, since True (i.e. key is in dict) is sorted after False. &gt;&gt;&gt; dicts = [{1:2, 3:4}, {3:4}, {5:6, 7:8}] &gt;&gt;&gt; keys = [5, 3, 1] &gt;&gt;&gt; sorted(dicts, key=lambda d: [k in d for k in keys], reverse=True) [{5: 6, 7: 8}, {1: 2, 3: 4}, {3: 4}] This is using all the keys to break ties, i.e. in above example, there are two dicts that have the key 3, but one also has the key 1, so this one is sorted second. Remove a key from a dict Source: stackoverflow.com To delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop(): my_dict.pop('key', None) This will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised. To delete a key that is guaranteed to exist, you can also use del my_dict['key'] This will raise a KeyError if the key is not in the dictionary. Variables Create dynamic variable names Using for loop The creation of a dynamic variable name in Python can be achieved with the help of iteration. Along with the for loop, the globals() function will also be used in this method. The globals() method in Python provides the output as a dictionary of the current global symbol table. The following code uses the for loop and the globals() method to create a dynamic variable name in Python. for n in range(0, 7): globals()['strg%s' % n] = 'Hello' # strg0 = 'Hello', strg1 = 'Hello' ... strg6 = 'Hello' for x in range(0, 7): globals()[f\"variable1{x}\"] = f\"Hello the variable number {x}!\" print(variable5) Output: Hello from variable number 5! Using a dictionary A dictionary is one of the four built-in data-types provided by Python along with tuple, list, and set. It is used to store data in the form of key: value pairs. A dictionary is both ordered (in Python 3.7 and above) and mutable. It is written with curly brackets {}. In addition to this, dictionaries cannot have any duplicates. A dictionary has both a key and value, so it is easy to create a dynamic variable name using dictionaries. The following code uses a dictionary to create a dynamic variable name in python. var = \"a\" val = 4 dict1 = {var: val} print(dict1[\"a\"]) Although the creation of a dynamic variable name is possible in Python, It is needless and unnecessary as data in Python is created dynamically. Python references the objects in the code. If the reference of the object exists, then the object itself exists. Creating a variable in this way is not recommended."
  },"/Materials-Research-Notes/Chap1-Coding/Vim_cheatsheet/": {
    "title": "Vim Cheatsheet",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap1-Coding/Vim_cheatsheet/",
    "body": "Cliped from vim.rtorr.com Global :h[elp] keyword - open help for keyword :sav[eas] file - save file as :clo[se] - close current pane :ter[minal] - open a terminal window K - open man page for word under the cursor Tip Run vimtutor in a terminal to learn the first Vim commands. Cursor movement h - move cursor left j - move cursor down k - move cursor up l - move cursor right gj - move cursor down (multi-line text) gk - move cursor up (multi-line text) H - move to top of screen M - move to middle of screen L - move to bottom of screen w - jump forwards to the start of a word W - jump forwards to the start of a word (words can contain punctuation) e - jump forwards to the end of a word E - jump forwards to the end of a word (words can contain punctuation) b - jump backwards to the start of a word B - jump backwards to the start of a word (words can contain punctuation) ge - jump backwards to the end of a word gE - jump backwards to the end of a word (words can contain punctuation) % - move to matching character (default supported pairs: ‘()’, ‘{}’, ‘[]’ - use :h matchpairs in vim for more info) 0 - jump to the start of the line ^ - jump to the first non-blank character of the line $ - jump to the end of the line g_ - jump to the last non-blank character of the line gg - go to the first line of the document G - go to the last line of the document 5gg or 5G - go to line 5 gd - move to local declaration gD - move to global declaration fx - jump to next occurrence of character x tx - jump to before next occurrence of character x Fx - jump to the previous occurrence of character x Tx - jump to after previous occurrence of character x ; - repeat previous f, t, F or T movement , - repeat previous f, t, F or T movement, backwards } - jump to next paragraph (or function/block, when editing code) { - jump to previous paragraph (or function/block, when editing code) zz - center cursor on screen zt - position cursor on top of the screen zb - position cursor on bottom of the screen Ctrl + e - move screen down one line (without moving cursor) Ctrl + y - move screen up one line (without moving cursor) Ctrl + b - move back one full screen Ctrl + f - move forward one full screen Ctrl + d - move forward 1/2 a screen Ctrl + u - move back 1/2 a screen Tip Prefix a cursor movement command with a number to repeat it. For example, 4j moves down 4 lines. Insert mode - inserting/appending text i - insert before the cursor I - insert at the beginning of the line a - insert (append) after the cursor A - insert (append) at the end of the line o - append (open) a new line below the current line O - append (open) a new line above the current line ea - insert (append) at the end of the word Ctrl + h - delete the character before the cursor during insert mode Ctrl + w - delete word before the cursor during insert mode Ctrl + j - begin new line during insert mode Ctrl + t - indent (move right) line one shiftwidth during insert mode Ctrl + d - de-indent (move left) line one shiftwidth during insert mode Ctrl + n - insert (auto-complete) next match before the cursor during insert mode Ctrl + p - insert (auto-complete) previous match before the cursor during insert mode Ctrl + rx - insert the contents of register x Ctrl + ox - Temporarily enter normal mode to issue one normal-mode command x. Esc or Ctrl + c - exit insert mode Editing r - replace a single character. R - replace more than one character, until ESC is pressed. J - join line below to the current one with one space in between gJ - join line below to the current one without space in between gwip - reflow paragraph g~ - switch case up to motion gu - change to lowercase up to motion gU - change to uppercase up to motion cc - change (replace) entire line c$ or C - change (replace) to the end of the line ciw - change (replace) entire word cw or ce - change (replace) to the end of the word s - delete character and substitute text S - delete line and substitute text (same as cc) xp - transpose two letters (delete and paste) u - undo U - restore (undo) last changed line Ctrl + r - redo . - repeat last command Marking text (visual mode) v - start visual mode, mark lines, then do a command (like y-yank) V - start linewise visual mode o - move to other end of marked area Ctrl + v - start visual block mode O - move to other corner of block aw - mark a word ab - a block with () aB - a block with {} at - a block with &lt;&gt; tags ib - inner block with () iB - inner block with {} it - inner block with &lt;&gt; tags Esc or Ctrl + c - exit visual mode Tip Instead of b or B one can also use ( or { respectively. Visual commands shift text right &lt; - shift text left y - yank (copy) marked text d - delete marked text ~ - switch case u - change marked text to lowercase U - change marked text to uppercase Registers :reg[isters] - show registers content “xy - yank into register x “xp - paste contents of register x “+y - yank into the system clipboard register “+p - paste from the system clipboard register Tip Registers are being stored in ~/.viminfo, and will be loaded again on next restart of vim. Tip Special registers:  0 - last yank  ” - unnamed register, last delete or yank  % - current file name  # - alternate file name  * - clipboard contents (X11 primary)  + - clipboard contents (X11 clipboard)  / - last search pattern  : - last command-line  . - last inserted text  - - last small (less than a line) delete  = - expression register  _ - black hole register Marks and positions :marks - list of marks ma - set current position for mark A `a - jump to position of mark A y`a - yank text to position of mark A `0 - go to the position where Vim was previously exited `” - go to the position when last editing this file `. - go to the position of the last change in this file `` - go to the position before the last jump :ju[mps] - list of jumps Ctrl + i - go to newer position in jump list Ctrl + o - go to older position in jump list :changes - list of changes g, - go to newer position in change list g; - go to older position in change list Ctrl + ] - jump to the tag under cursor Tip To jump to a mark you can either use a backtick (`) or an apostrophe (‘). Using an apostrophe jumps to the beginning (first non-blank) of the line holding the mark. Macros qa - record macro a q - stop recording macro @a - run macro a @@ - rerun last run macro Cut and paste yy - yank (copy) a line 2yy - yank (copy) 2 lines yw - yank (copy) the characters of the word from the cursor position to the start of the next word yiw - yank (copy) word under the cursor yaw - yank (copy) word under the cursor and the space after or before it y$ or Y - yank (copy) to end of line p - put (paste) the clipboard after cursor P - put (paste) before cursor gp - put (paste) the clipboard after cursor and leave cursor after the new text gP - put (paste) before cursor and leave cursor after the new text dd - delete (cut) a line 2dd - delete (cut) 2 lines dw - delete (cut) the characters of the word from the cursor position to the start of the next word diw - delete (cut) word under the cursor daw - delete (cut) word under the cursor and the space after or before it :3,5d - delete lines starting from 3 to 5 Tip You can also use the following characters to specify the range: e.g. :.,$d - From the current line to the end of the file :.,1d - From the current line to the beginning of the file :10,$d - From the 10th line to the beginning of the file :g/{pattern}/d - delete all lines containing pattern :g!/{pattern}/d - delete all lines not containing pattern d$ or D - delete (cut) to the end of the line x - delete (cut) character Indent text indent (move right) line one shiftwidth « - de-indent (move left) line one shiftwidth % - indent a block with () or {} (cursor on brace) &lt;% - de-indent a block with () or {} (cursor on brace) ib - indent inner block with () at - indent a block with &lt;&gt; tags 3== - re-indent 3 lines =% - re-indent a block with () or {} (cursor on brace) =iB - re-indent inner block with {} gg=G - re-indent entire buffer ]p - paste and adjust indent to current line Exiting :w - write (save) the file, but don’t exit :w !sudo tee % - write out the current file using sudo :wq or :x or ZZ - write (save) and quit :q - quit (fails if there are unsaved changes) :q! or ZQ - quit and throw away unsaved changes :wqa - write (save) and quit on all tabs Search and replace /pattern - search for pattern ?pattern - search backward for pattern \\vpattern - ‘very magic’ pattern: non-alphanumeric characters are interpreted as special regex symbols (no escaping needed) n - repeat search in same direction N - repeat search in opposite direction :%s/old/new/g - replace all old with new throughout file :%s/old/new/gc - replace all old with new throughout file with confirmations :noh[lsearch] - remove highlighting of search matches Search in multiple files :vim[grep] /pattern/ {{file}} - search for pattern in multiple files e.g. :vim[grep] /foo/ */ :cn[ext] - jump to the next match :cp[revious] - jump to the previous match :cope[n] - open a window containing the list of matches :ccl[ose] - close the quickfix window Tabs :tabnew or :tabnew {page.words.file} - open a file in a new tab Ctrl + wT - move the current split window into its own tab gt or :tabn[ext] - move to the next tab gT or :tabp[revious] - move to the previous tab #gt - move to tab number # :tabm[ove] # - move current tab to the #th position (indexed from 0) :tabc[lose] - close the current tab and all its windows :tabo[nly] - close all tabs except for the current one :tabdo command - run the command on all tabs (e.g. :tabdo q - closes all opened tabs)"
  },"/Materials-Research-Notes/Chap1-Coding/numpy-tricks/": {
    "title": "Numpy - practical tricks and code snipets",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap1-Coding/numpy-tricks/",
    "body": "Boolean Invert The ~ operator can be used as a shorthand for np.invert on ndarrays. x1 = np.array([True, False]) ~x1 array([False, True]) np.histogram and np.digitize The output of np.histogram actually has 10 bins; the last (right-most) bin includes the greatest element because its right edge is inclusive (unlike for other bins). The np.digitize method doesn’t make such an exception (since its purpose is different) so the largest element(s) of the list get placed into an extra bin. To get the bin assignments that are consistent with histogram, just clamp the output of digitize by the number of bins, using fmin. A = range(1, 94) bin_count = 10 hist = np.histogram(A, bins = bin_count) np.fmin(np.digitize(A, hist[1]), bin_count) Output: array([ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
  },"/Materials-Research-Notes/Chap1-Coding/python-sequences/": {
    "title": "Python - sequences",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap1-Coding/python-sequences/",
    "body": "Overview Types in terms of structure container sequences Can hold items of different types, including nested containers. E.g. list, tuple, and collections.deque. flat sequences Hold items of one simple type. E.g. str, bytes, and array.array. A container sequence holds references to the objects it contains which may be of any type. A flat sequence stores the value of its contents in its own memory space, not as distinct Python objects. Note Every Python object in memory has a header with metadata. E.g. the simplest object float ob_refcnt: metadata, reference count ob_type: metadata, a pointer to the object's type ob_fval: value, a C double holding the value of the float Types in terms of mutability mutable sequences E.g. list, bytearray, and collections.deque. immutable sequences E.g. tuple, str and bytes. &gt;&gt;&gt; from collections import abc &gt;&gt;&gt; issubclass(tuple, abc.Sequence) True &gt;&gt;&gt; issubclass(list, abc.MutableSequence) True"
  },"/Materials-Research-Notes/Chap2-ML/GBR/": {
    "title": "ML - Gradient Boosting Algorithm",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap2-ML/GBR/",
    "body": "Introduction Gradient boosting is one of the variants of ensemble methods where you create multiple weak models and combine them to get better performance as a whole. machinelearningmastery.com Gradient boosting is one of the most powerful techniques for building predictive models. The Origin of Boosting The idea of boosting came out of the idea of whether a weak learner can be modified to become better. Michael Kearns articulated the goal as the “Hypothesis Boosting Problem” stating the goal from a practical standpoint as: … an efficient algorithm for converting relatively poor hypotheses into very good hypotheses — Thoughts on Hypothesis Boosting [PDF], 1988 A weak hypothesis or weak learner is defined as one whose performance is at least slightly better than random chance. These ideas built upon Leslie Valiant’s  work on distribution free or Probably Approximately Correct (PAC) learning, a framework for investigating the complexity of machine learning problems. Hypothesis boosting was the idea of filtering observations, leaving those observations that the weak learner can handle and focusing on developing new weak learns to handle the remaining difficult observations. The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. … Note, however, it is not obvious at all how this can be done — Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World, page 152, 2013 AdaBoost the First Boosting Algorithm The first realization of boosting that saw great success in application was Adaptive Boosting or AdaBoost for short. Boosting refers to this general problem of producing a very accurate prediction rule by combining rough and moderately inaccurate rules-of-thumb. — A decision-theoretic generalization of on-line learning and an application to boosting [PDF], 1995 The weak learners in AdaBoost are decision trees with a single split, called decision stumps for their shortness. AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. This means that samples that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies these samples — Applied Predictive Modeling, 2013 Predictions are made by majority vote of the weak learners’ predictions, weighted by their individual accuracy. The most successful form of the AdaBoost algorithm was for binary classification problems and was called AdaBoost.M1. You can learn more about the AdaBoost algorithm in the post: Boosting and AdaBoost for Machine Learning. Generalization of AdaBoost as Gradient Boosting AdaBoost and related algorithms were recast in a statistical framework first by Breiman calling them ARCing algorithms. Arcing is an acronym for Adaptive Reweighting and Combining. Each step in an arcing algorithm consists of a weighted minimization followed by a recomputation of [the classifiers] and [weighted input]. — Prediction Games and Arching Algorithms [PDF], 1997 This framework was further developed by Friedman and called Gradient Boosting Machines. Later called just gradient boosting or gradient tree boosting. The statistical framework cast boosting as a numerical optimization problem where the objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure. This class of algorithms were described as a stage-wise additive model. This is because one new weak learner is added at a time and existing weak learners in the model are frozen and left unchanged. Note that this stagewise strategy is different from stepwise approaches that readjust previously entered terms when new ones are added. — Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999 The generalization allowed arbitrary differentiable loss functions to be used, expanding the technique beyond binary classification problems to support regression, multi-class classification and more. How Gradient Boosting Works Gradient boosting involves three elements: A loss function to be optimized. A weak learner to make predictions. An additive model to add weak learners to minimize the loss function. 1. Loss Function The loss function used depends on the type of problem being solved. It must be differentiable, but many standard loss functions are supported and you can define your own. For example, regression may use a squared error and classification may use logarithmic loss. A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used. 2. Weak Learner Decision trees are used as the weak learner in gradient boosting. Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions. Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss. Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels. It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes. This is to ensure that the learners remain weak, but can still be constructed in a greedy manner. 3. Additive Model Trees are added one at a time, and existing trees in the model are not changed. A gradient descent procedure is used to minimize the loss when adding trees. Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error. Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss. Generally this approach is called functional gradient descent or gradient descent with functions. One way to produce a weighted combination of classifiers which optimizes [the cost] is by gradient descent in function space — Boosting Algorithms as Gradient Descent in Function Space [PDF], 1999 The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model. A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset. Improvements to Basic Gradient Boosting Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting. In this this section we will look at 4 enhancements to basic gradient boosting: Tree Constraints Shrinkage Random sampling Penalized Learning 1. Tree Constraints It is important that the weak learners have skill but remain weak. There are a number of ways that the trees can be constrained. A good general heuristic is that the more constrained tree creation is, the more trees you will need in the model, and the reverse, where less constrained individual trees, the fewer trees that will be required. Below are some constraints that can be imposed on the construction of decision trees: Number of trees, generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed. Tree depth, deeper trees are more complex trees and shorter trees are preferred. Generally, better results are seen with 4-8 levels. Number of nodes or number of leaves, like depth, this can constrain the size of the tree, but is not constrained to a symmetrical structure if other constraints are used. Number of observations per split imposes a minimum constraint on the amount of training data at a training node before a split can be considered Minimim improvement to loss is a constraint on the improvement of any split added to a tree. 2. Weighted Updates The predictions of each tree are added together sequentially. The contribution of each tree to this sum can be weighted to slow down the learning by the algorithm. This weighting is called a shrinkage or a learning rate. Each update is simply scaled by the value of the “learning rate parameter v” — Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999 The effect is that learning is slowed down, in turn require more trees to be added to the model, in turn taking longer to train, providing a configuration trade-off between the number of trees and learning rate. Decreasing the value of v [the learning rate] increases the best value for M [the number of trees]. — Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999 It is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. — Stochastic Gradient Boosting [PDF], 1999 3. Stochastic Gradient Boosting A big insight into bagging ensembles and random forest was allowing trees to be greedily created from subsamples of the training dataset. This same benefit can be used to reduce the correlation between the trees in the sequence in gradient boosting models. This variation of boosting is called stochastic gradient boosting. at each iteration a subsample of the training data is drawn at random (without replacement) from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to fit the base learner. — Stochastic Gradient Boosting [PDF], 1999 A few variants of stochastic boosting that can be used: Subsample rows before creating each tree. Subsample columns before creating each tree Subsample columns before considering each split. Generally, aggressive sub-sampling such as selecting only 50% of the data has shown to be beneficial. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling — XGBoost: A Scalable Tree Boosting System, 2016 4. Penalized Gradient Boosting Additional constraints can be imposed on the parameterized trees in addition to their structure. Classical decision trees like CART are not used as weak learners, instead a modified form called a regression tree is used that has numeric values in the leaf nodes (also called terminal nodes). The values in the leaves of the trees can be called weights in some literature. As such, the leaf weight values of the trees can be regularized using popular regularization functions, such as: L1 regularization of weights. L2 regularization of weights. The additional regularization term helps to smooth the final learnt weights to avoid over-fitting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions. — XGBoost: A Scalable Tree Boosting System, 2016 Gradient Boosting Resources Gradient boosting is a fascinating algorithm and I am sure you want to go deeper. This section lists various resources that you can use to learn more about the gradient boosting algorithm. Gradient Boosting Videos Gradient Boosting Machine Learning, Trevor Hastie, 2014 Gradient Boosting, Alexander Ihler, 2012 GBM, John Mount, 2015 Learning: Boosting, MIT 6.034 Artificial Intelligence, 2010 xgboost: An R package for Fast and Accurate Gradient Boosting, 2016 XGBoost: A Scalable Tree Boosting System, Tianqi Chen, 2016 Gradient Boosting in Textbooks Section 8.2.3 Boosting, page 321, An Introduction to Statistical Learning: with Applications in R. Section 8.6 Boosting, page 203, Applied Predictive Modeling. Section 14.5 Stochastic Gradient Boosting, page 390, Applied Predictive Modeling. Section 16.4 Boosting, page 556, Machine Learning: A Probabilistic Perspective Chapter 10 Boosting and Additive Trees, page 337, The Elements of Statistical Learning: Data Mining, Inference, and Prediction Gradient Boosting Papers Thoughts on Hypothesis Boosting [PDF], Michael Kearns, 1988 A decision-theoretic generalization of on-line learning and an application to boosting [PDF], 1995 Arcing the edge [PDF], 1998 Stochastic Gradient Boosting [PDF], 1999 Boosting Algorithms as Gradient Descent in Function Space [PDF], 1999 Gradient Boosting Slides Introduction to Boosted Trees, 2014 A Gentle Introduction to Gradient Boosting, Cheng Li Gradient Boosting Web Pages Boosting (machine learning) Gradient boosting Gradient Tree Boosting in scikit-learn Summary In this post you discovered the gradient boosting algorithm for predictive modeling in machine learning. Specifically, you learned: The history of boosting in learning theory and AdaBoost. How the gradient boosting algorithm works with a loss function, weak learners and an additive model. How to improve the performance of gradient boosting with regularization. Tomonori Masui’ post Cliped from Towards Data Science All You Need to Know about Gradient Boosting Algorithm − Part 1. Regression Section 1. Algorithm with an Example Gradient boosting is one of the most popular machine learning algorithms for tabular datasets. It is powerful enough to find any nonlinear relationship between your model target and features and has great usability that can deal with missing values, outliers, and high cardinality categorical values on your features without any special treatment. While you can build barebone gradient boosting trees using some popular libraries such as XGBoost or LightGBM without knowing any details of the algorithm, you still want to know how it works when you start tuning hyper-parameters, customizing the loss functions, etc., to get better quality on your model. This article aims to provide you with all the details about the algorithm, specifically its regression algorithm, including its math and Python code from scratch. If you are more interested in the classification algorithm, please look at Part 2. Gradient boosting is one of the variants of ensemble methods where you create multiple weak models and combine them to get better performance as a whole. In this section, we are building gradient boosting regression trees step by step using the below sample which has a nonlinear relationship between $x$ and $y$ to intuitively understand how it works (all the pictures below are created by the author). Sample for a regression problem. The first step is making a very naive prediction on the target $y$. We make the initial prediction $F_0$ as an overall average of $y$: Initial prediction: $F_0 = \\text{mean}(y)$. You might feel using the mean for the prediction is silly, but don’t worry. We will improve our prediction as we add more weak models to it. To improve our prediction, we will focus on the residuals (i.e. prediction errors) from the first step because that is what we want to minimize to get a better prediction. The residuals $r₁$ are shown as the vertical blue lines in the figure below. To minimize these residuals, we are building a regression tree model with $x$ as its feature and the residuals $r_1 = y − \\text{mean}(y)$ as its target. The reasoning behind that is if we can find some patterns between $x$ and $r_1$ by building the additional weak model, we can reduce the residuals by utilizing it. To simplify the demonstration, we are building very simple trees each of that only has one split and two terminal nodes which is called “stump”. Please note that gradient boosting trees usually have a little deeper trees such as ones with 8 to 32 terminal nodes. Here we are creating the first tree predicting the residuals with two different values $\\gamma _1 = {6.0, − 5.9}$(we are using $\\gamma$ (gamma) to denotes the prediction). This prediction $\\gamma _1$ is added to our initial prediction $F_0$ to reduce the residuals. In fact, gradient boosting algorithm does not simply add $\\gamma$ to $F$ as it makes the model overfit to the training data. Instead, $\\gamma$ is scaled down by learning rate $\\nu$ which ranges between 0 and 1, and then added to $F$. \\[F_1 = F_0 + \\nu \\cdot \\gamma _1\\] In this example, we use a relatively big learning rate $\\nu = 0.9$ to make the optimization process easier to understand, but it is usually supposed to be a much smaller value such as 0.1. After the update, our combined prediction $F_1$ becomes: \\[F_1 = \\begin{cases} F_0 + \\nu \\cdot 6.0 &amp;\\text{if } x \\leq 49.5 \\\\ F_0 - \\nu \\cdot 5.9 &amp;\\text{otherwise} \\end{cases}\\] Now, the updated residuals $r_2$ looks like this: In the next step, we are creating a regression tree again using the same $x$ as the feature and the updated residuals $r_2$ as its target. Here is the created tree: Then, we are updating our previous combined prediction $F_1$ with the new tree prediction $\\gamma _2$. We iterate these steps until the model prediction stops improving. The figures below show the optimization process from 0 to 6 iterations. You can see the combined prediction $F_m$ is getting more closer to our target $y$ as we add more trees into the combined model. This is how gradient boosting works to predict complex targets by combining multiple weak models. Section 2. Math In this section, we are diving into the math details of the algorithm. Here is the whole algorithm in math formulas. Source: adapted from Wikipedia and Friedman’s paper Let’s demystify this line by line. Step 1 The first step is creating an initial constant value prediction $F_0$. $L$ is the loss function and it is squared loss in our regression case. $argmin$ means we are searching for the value $\\gamma$ that minimizes $\\sum L(y_i, \\gamma)$. Let’s compute the value $\\gamma$ by using our actual loss function. To find $\\gamma$ that minimizes $ΣL$, we are taking a derivative of $\\sum L$ with respect to $\\gamma$. And we are finding $\\gamma$ that makes $\\frac {\\partial \\sum L} {\\gamma}$ equal to 0. It turned out that the value $\\gamma$ that minimizes $ΣL$ is the mean of $y$. This is why we used $y$ mean for our initial prediction $F₀$ in the last section. Step2 The whole step2 processes from 2–1 to 2–4 are iterated $M$ times. $M$ denotes the number of trees we are creating and the small $m$ represents the index of each tree. Step 2–1 We are calculating residuals $r_i 𝑚$ by taking a derivative of the loss function with respect to the previous prediction $F_{m-1}$ and multiplying it by −1. As you can see in the subscript index, $r_i m$ is computed for each single sample $i$. Some of you might be wondering why we are calling this $rᵢ𝑚$ residuals. This value is actually negative gradient that gives us guidance on the directions ($+/−$) and the magnitude in which the loss function can be minimized. You will see why we are calling it residuals shortly. By the way, this technique where you use a gradient to minimize the loss on your model is very similar to gradient descent technique which is typically used to optimize neural networks. (In fact, they are slightly different from each other. If you are interested, please look at this article detailing that topic.) Let’s compute the residuals here. $F{m-1}$ in the equation means the prediction from the previous step. In this first iteration, it is $F_0$. We are solving the equation for residuals $r_i 𝑚$. We can take 2 out of it as it is just a constant. That leaves us $r_{im} = y_i − F_{m-1}$. You might now see why we call it residuals. This also gives us interesting insight that the negative gradient that provides us the direction and the magnitude to which the loss is minimized is actually just residuals. Step 2–2 $j$ represents a terminal node (i.e. leave) in the tree, $m$ denotes the tree index, and capital $J$ means the total number of leaves. Step 2–3 We are searching for $\\gamma {jm}$ that minimizes the loss function on each terminal node $j$. $\\sum x_i R{jm} L$ means we are aggregating the loss on all the sample $x_i$s that belong to the terminal node $R_{im}$. Let’s plugin the loss function into the equation. Then, we are finding $\\gamma _{jm}$ that makes the derivative of $\\sum (*)$ equals zero. Please note that $n_j$ means the number of samples in the terminal node $j$. This means the optimal $\\gamma {jm}$ that minimizes the loss function is the average of the residuals $r{im}$ in the terminal node $R_j m$. In other words, $\\gamma _{jm}$ is the regular prediction values of regression trees that are the average of the target values (in our case, residuals) in each terminal node. Step 2–4 In the final step, we are updating the prediction of the combined model $F_m$. $\\gamma {jm} 1(x ∈ R{jm})$ means that we pick the value $\\gamma {im}$ if a given $x$ falls in a terminal node $R{jm}$. As all the terminal nodes are exclusive, any given single $x$ falls into only a single terminal node and corresponding $\\gammaⱼ𝑚$ is added to the previous prediction $F_{m-1}_$ and it makes updated prediction $F_m$. As mentioned in the previous section, $\\nu$ is learning rate ranging between 0 and 1 which controls the degree of contribution of the additional tree prediction $\\gamma$ to the combined prediction $F_m$. A smaller learning rate reduces the effect of the additional tree prediction, but it basically also reduces the chance of the model overfitting to the training data. Now we have gone through the whole steps. To get the best model performance, we want to iterate step 2 $M$ times, which means adding $M$ trees to the combined model. In reality, you might often want to add more than 100 trees to get the best model performance. Some of you might feel that all those maths are unnecessarily complex as the previous section showed the basic idea in a much simpler way without all those complications. The reason behind it is that gradient boosting is designed to be able to deal with any loss functions as long as it is differentiable and the maths we reviewed is a generalized form of gradient boosting algorithm with that flexibility. That makes the formula a little complex, but it is the beauty of the algorithm as it has huge flexibility and convenience to work on a variety of types of problems. For example, if your problem requires absolute loss instead of squared loss, you can just replace the loss function and the whole algorithm works as it is as defined above. In fact, popular gradient boosting implementations such as XGBoost or LightGBM have a wide variety of loss functions, so you can choose whatever loss functions that suit your problem (see the various loss functions available in XGBoost or LightGBM). Section 3. Code In this section, we are translating the maths we just reviewed into a viable python code to help us understand the algorithm further. The code is mostly derived from Matt Bowers’ implementation, so all credit goes to his work. We are using DecisionTreeRegressor from scikit-learn to build trees which helps us just focus on the gradient boosting algorithm itself instead of the tree algorithm. We are imitating scikit-learn style implementation where you train the model with fit method and make predictions with predict method. class CustomGradientBoostingRegressor: def __init__(self, learning_rate, n_estimators, max_depth=1): self.learning_rate = learning_rate self.n_estimators = n_estimators self.max_depth = max_depth self.trees = [] def fit(self, X, y): self.F0 = y.mean() Fm = self.F0 for _ in range(self.n_estimators): r = y - Fm tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=0) tree.fit(X, r) gamma = tree.predict(X) Fm += self.learning_rate * gamma self.trees.append(tree) def predict(self, X): Fm = self.F0 for i in range(self.n_estimators): Fm += self.learning_rate * self.trees[i].predict(X) return Fm Please note that all the trained trees are stored in self.trees list object and it is retrieved when we make predictions with predict method. Next, we are checking if our CustomGradientBoostingRegressor performs as the same as GradientBoostingRegressor from scikit-learn by looking at their RMSE on our data. from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error custom_gbm = CustomGradientBoostingRegressor( n_estimators=20, learning_rate=0.1, max_depth=1 ) custom_gbm.fit(x, y) custom_gbm_rmse = mean_squared_error(y, custom_gbm.predict(x), squared=False) print(f\"Custom GBM RMSE:{custom_gbm_rmse:.15f}\") sklearn_gbm = GradientBoostingRegressor( n_estimators=20, learning_rate=0.1, max_depth=1 ) sklearn_gbm.fit(x, y) sklearn_gbm_rmse = mean_squared_error(y, sklearn_gbm.predict(x), squared=False) print(f\"Scikit-learn GBM RMSE:{sklearn_gbm_rmse:.15f}\") As you can see in the output above, both models have exactly the same RMSE. The algorithm we have reviewed in this post is just one of the options of gradient boosting algorithm that is specific to regression problems with squared loss. If you are also interested in the classification algorithm, please look at Part 2. There are also some other great resources if you want further details of the algorithm: StatQuest, Gradient Boost Part1 and Part 2 This is a YouTube video explaining GB regression algorithm with great visuals in a beginner-friendly way. Terence Parr and Jeremy Howard, How to explain gradient boostingThis article also focuses on GB regression. It explains how the algorithms differ between squared loss and absolute loss. Jerome Friedman, Greedy Function Approximation: A Gradient Boosting MachineThis is the original paper from Friedman. While it is a little hard to understand, it surely shows the flexibility of the algorithm where he shows a generalized algorithm that can deal with any types of problem having a differentiable loss function. You can also look at the full Python code in the Google Colab link or the Github link below. Jerome Friedman, Greedy Function Approximation: A Gradient Boosting Machine Terence Parr and Jeremy Howard, How to explain gradient boosting Matt Bowers, How to Build a Gradient Boosting Machine from Scratch Wikipedia, Gradient boosting"
  },"/Materials-Research-Notes/Chap2-ML/Out_liers/": {
    "title": "ML - handle outliers",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap2-ML/Out_liers/",
    "body": "Source: machinelearningmastery.com The presence of outliers in a classification or regression dataset can result in a poor fit and lower predictive modeling performance. Identifying and removing outliers is challenging with simple statistical methods for most machine learning datasets given the large number of input variables. Instead, automatic outlier detection methods can be used in the modeling pipeline and compared, just like other data preparation transforms that may be applied to the dataset. Outlier Detection and Removal Outliers are observations in a dataset that don’t fit in some way. Perhaps the most common or familiar type of outlier is the observations that are far from the rest of the observations or the center of mass of observations. This is easy to understand when we have one or two variables and we can visualize the data as a histogram or scatter plot, although it becomes very challenging when we have many input variables defining a high-dimensional input feature space. In this case, simple statistical methods for identifying outliers can break down, such as methods that use standard deviations or the interquartile range. It can be important to identify and remove outliers from data when training machine learning algorithms for predictive modeling. Outliers can skew statistical measures and data distributions, providing a misleading representation of the underlying data and relationships. Removing outliers from training data prior to modeling can result in a better fit of the data and, in turn, more skillful predictions. Thankfully, there are a variety of automatic model-based methods for identifying outliers in input data. Importantly, each method approaches the definition of an outlier is slightly different ways, providing alternate approaches to preparing a training dataset that can be evaluated and compared, just like any other data preparation step in a modeling pipeline. Dataset and Performance Baseline Before we dive into automatic outlier detection methods, let’s first select a standard machine learning dataset that we can use as the basis for our investigation. This will provide the context for exploring the outlier identification and removal method of data preparation in the next section. House Price Regression Dataset We will use the house price regression dataset. This dataset has 13 input variables that describe the properties of the house and suburb and requires the prediction of the median value of houses in the suburb in thousands of dollars. You can learn more about the dataset here: House Price Dataset (housing.csv) House Price Dataset Description (housing.names) No need to download the dataset as we will download it automatically as part of our worked examples. Open the dataset and review the raw data. The first few rows of data are listed below. We can see that it is a regression predictive modeling problem with numerical input variables, each of which has different scales. 0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98,24.000.02731,0.00,7.070,0,0.4690,6.4210,78.90,4.9671,2,242.0,17.80,396.90,9.14,21.600.02729,0.00,7.070,0,0.4690,7.1850,61.10,4.9671,2,242.0,17.80,392.83,4.03,34.700.03237,0.00,2.180,0,0.4580,6.9980,45.80,6.0622,3,222.0,18.70,394.63,2.94,33.400.06905,0.00,2.180,0,0.4580,7.1470,54.20,6.0622,3,222.0,18.70,396.90,5.33,36.20... The dataset has many numerical input variables that have unknown and complex relationships. We don’t know that outliers exist in this dataset, although we may guess that some outliers may be present. The example below loads the dataset and splits it into the input and output columns, splits it into train and test datasets, then summarizes the shapes of the data arrays. # load and summarize the datasetfrom pandas import read_csvfrom sklearn.model_selection import train_test_split# load the dataseturl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'df = read_csv(url, header=None)# retrieve the arraydata = df.values# split into input and output elementsX, y = data[:, :-1], data[:, -1]# summarize the shape of the datasetprint(X.shape, y.shape)# split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)# summarize the shape of the train and test setsprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape) Running the example, we can see that the dataset was loaded correctly and that there are 506 rows of data with 13 input variables and a single target variable. The dataset is split into train and test sets with 339 rows used for model training and 167 for model evaluation. (506, 13) (506,)(339, 13) (167, 13) (339,) (167,) Next, let’s evaluate a model on this dataset and establish a baseline in performance. Baseline Model Performance It is a regression predictive modeling problem, meaning that we will be predicting a numeric value. All input variables are also numeric. In this case, we will fit a linear regression algorithm and evaluate model performance by training the model on the test dataset and making a prediction on the test data and evaluate the predictions using the mean absolute error (MAE). The complete example of evaluating a linear regression model on the dataset is listed below. 12345678910111213141516171819202122# evaluate model on the raw datasetfrom pandas import read_csvfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_absolute_error# load the dataseturl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'df = read_csv(url, header=None)# retrieve the arraydata = df.values# split into input and output elementsX, y = data[:, :-1], data[:, -1]# split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)# fit the modelmodel = LinearRegression()model.fit(X_train, y_train)# evaluate the modelyhat = model.predict(X_test)# evaluate predictionsmae = mean_absolute_error(y_test, yhat)print('MAE: %.3f' % mae) Running the example fits and evaluates the model, then reports the MAE. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the model achieved a MAE of about 3.417. This provides a baseline in performance to which we can compare different outlier identification and removal procedures. Automatic Outlier Detection The scikit-learn library provides a number of built-in automatic methods for identifying outliers in data. We will review four methods and compare their performance on the house price dataset. Each method will be defined, then fit on the training dataset. The fit model will then predict which examples in the training dataset are outliers and which are not (so-called inliers). The outliers will then be removed from the training dataset, then the model will be fit on the remaining examples and evaluated on the entire test dataset. It would be invalid to fit the outlier detection method on the entire training dataset as this would result in data leakage. That is, the model would have access to data (or information about the data) in the test set not used to train the model. This may result in an optimistic estimate of model performance. We could attempt to detect outliers on “new data” such as the test set prior to making a prediction, but then what do we do if outliers are detected? One approach might be to return a “None” indicating that the model is unable to make a prediction on those outlier cases. This might be an interesting extension to explore that may be appropriate for your project. Isolation Forest Isolation Forest, or iForest for short, is a tree-based anomaly detection algorithm. It is based on modeling the normal data in such a way as to isolate anomalies that are both few in number and different in the feature space. … our proposed method takes advantage of two anomalies’ quantitative properties: i. they are the minority consisting of fewer instances and ii. they have attribute-values that are very different from those of normal instances. — Isolation Forest, 2008. The scikit-learn library provides an implementation of Isolation Forest in the IsolationForest class. Perhaps the most important hyperparameter in the model is the “contamination” argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1. ...# identify outliers in the training datasetiso = IsolationForest(contamination=0.1)yhat = iso.fit_predict(X_train) Once identified, we can remove the outliers from the training dataset. ...# select all rows that are not outliersmask = yhat != -1X_train, y_train = X_train[mask, :], y_train[mask] Tying this together, the complete example of evaluating the linear model on the housing dataset with outliers identified and removed with isolation forest is listed below. 123456789101112131415161718192021222324252627282930313233# evaluate model performance with outliers removed using isolation forestfrom pandas import read_csvfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.ensemble import IsolationForestfrom sklearn.metrics import mean_absolute_error# load the dataseturl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'df = read_csv(url, header=None)# retrieve the arraydata = df.values# split into input and output elementsX, y = data[:, :-1], data[:, -1]# split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)# summarize the shape of the training datasetprint(X_train.shape, y_train.shape)# identify outliers in the training datasetiso = IsolationForest(contamination=0.1)yhat = iso.fit_predict(X_train)# select all rows that are not outliersmask = yhat != -1X_train, y_train = X_train[mask, :], y_train[mask]# summarize the shape of the updated training datasetprint(X_train.shape, y_train.shape)# fit the modelmodel = LinearRegression()model.fit(X_train, y_train)# evaluate the modelyhat = model.predict(X_test)# evaluate predictionsmae = mean_absolute_error(y_test, yhat)print('MAE: %.3f' % mae) Running the example fits and evaluates the model, then reports the MAE. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that that model identified and removed 34 outliers and achieved a MAE of about 3.189, an improvement over the baseline that achieved a score of about 3.417. (339, 13) (339,)(305, 13) (305,)MAE: 3.189 Minimum Covariance Determinant If the input variables have a Gaussian distribution, then simple statistical methods can be used to detect outliers. For example, if the dataset has two input variables and both are Gaussian, then the feature space forms a multi-dimensional Gaussian and knowledge of this distribution can be used to identify values far from the distribution. This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. An efficient implementation of this technique for multivariate data is known as the Minimum Covariance Determinant, or MCD for short. The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. […] It also serves as a convenient and efficient tool for outlier detection. — Minimum Covariance Determinant and Extensions, 2017. The scikit-learn library provides access to this method via the EllipticEnvelope class. It provides the “contamination” argument that defines the expected ratio of outliers to be observed in practice. In this case, we will set it to a value of 0.01, found with a little trial and error. ...# identify outliers in the training datasetee = EllipticEnvelope(contamination=0.01)yhat = ee.fit_predict(X_train) Once identified, the outliers can be removed from the training dataset as we did in the prior example. Tying this together, the complete example of identifying and removing outliers from the housing dataset using the elliptical envelope (minimum covariant determinant) method is listed below. 123456789101112131415161718192021222324252627282930313233# evaluate model performance with outliers removed using elliptical envelopefrom pandas import read_csvfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.covariance import EllipticEnvelopefrom sklearn.metrics import mean_absolute_error# load the dataseturl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'df = read_csv(url, header=None)# retrieve the arraydata = df.values# split into input and output elementsX, y = data[:, :-1], data[:, -1]# split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)# summarize the shape of the training datasetprint(X_train.shape, y_train.shape)# identify outliers in the training datasetee = EllipticEnvelope(contamination=0.01)yhat = ee.fit_predict(X_train)# select all rows that are not outliersmask = yhat != -1X_train, y_train = X_train[mask, :], y_train[mask]# summarize the shape of the updated training datasetprint(X_train.shape, y_train.shape)# fit the modelmodel = LinearRegression()model.fit(X_train, y_train)# evaluate the modelyhat = model.predict(X_test)# evaluate predictionsmae = mean_absolute_error(y_test, yhat)print('MAE: %.3f' % mae) Running the example fits and evaluates the model, then reports the MAE. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the elliptical envelope method identified and removed only 4 outliers, resulting in a drop in MAE from 3.417 with the baseline to 3.388. (339, 13) (339,)(335, 13) (335,)MAE: 3.388 Local Outlier Factor A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space. This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality. The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers. We introduce a local outlier (LOF) for each object in the dataset, indicating its degree of outlier-ness. — LOF: Identifying Density-based Local Outliers, 2000. The scikit-learn library provides an implementation of this approach in the LocalOutlierFactor class. The model provides the “contamination” argument, that is the expected percentage of outliers in the dataset, be indicated and defaults to 0.1. ...# identify outliers in the training datasetlof = LocalOutlierFactor()yhat = lof.fit_predict(X_train) Tying this together, the complete example of identifying and removing outliers from the housing dataset using the local outlier factor method is listed below. 123456789101112131415161718192021222324252627282930313233# evaluate model performance with outliers removed using local outlier factorfrom pandas import read_csvfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.neighbors import LocalOutlierFactorfrom sklearn.metrics import mean_absolute_error# load the dataseturl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'df = read_csv(url, header=None)# retrieve the arraydata = df.values# split into input and output elementsX, y = data[:, :-1], data[:, -1]# split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)# summarize the shape of the training datasetprint(X_train.shape, y_train.shape)# identify outliers in the training datasetlof = LocalOutlierFactor()yhat = lof.fit_predict(X_train)# select all rows that are not outliersmask = yhat != -1X_train, y_train = X_train[mask, :], y_train[mask]# summarize the shape of the updated training datasetprint(X_train.shape, y_train.shape)# fit the modelmodel = LinearRegression()model.fit(X_train, y_train)# evaluate the modelyhat = model.predict(X_test)# evaluate predictionsmae = mean_absolute_error(y_test, yhat)print('MAE: %.3f' % mae) Running the example fits and evaluates the model, then reports the MAE. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the local outlier factor method identified and removed 34 outliers, the same number as isolation forest, resulting in a drop in MAE from 3.417 with the baseline to 3.356. Better, but not as good as isolation forest, suggesting a different set of outliers were identified and removed. (339, 13) (339,)(305, 13) (305,)MAE: 3.356 One-Class SVM The support vector machine, or SVM, algorithm developed initially for binary classification can be used for one-class classification. When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as One-Class SVM. … an algorithm that computes a binary function that is supposed to capture regions in input space where the probability density lives (its support), that is, a function such that most of the data will live in the region where the function is nonzero. — Estimating the Support of a High-Dimensional Distribution, 2001. Although SVM is a classification algorithm and One-Class SVM is also a classification algorithm, it can be used to discover outliers in input data for both regression and classification datasets. The scikit-learn library provides an implementation of one-class SVM in the OneClassSVM class. The class provides the “nu” argument that specifies the approximate ratio of outliers in the dataset, which defaults to 0.1. In this case, we will set it to 0.01, found with a little trial and error. ...# identify outliers in the training datasetee = OneClassSVM(nu=0.01)yhat = ee.fit_predict(X_train) Tying this together, the complete example of identifying and removing outliers from the housing dataset using the one class SVM method is listed below. 123456789101112131415161718192021222324252627282930313233# evaluate model performance with outliers removed using one class SVMfrom pandas import read_csvfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.svm import OneClassSVMfrom sklearn.metrics import mean_absolute_error# load the dataseturl = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'df = read_csv(url, header=None)# retrieve the arraydata = df.values# split into input and output elementsX, y = data[:, :-1], data[:, -1]# split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)# summarize the shape of the training datasetprint(X_train.shape, y_train.shape)# identify outliers in the training datasetee = OneClassSVM(nu=0.01)yhat = ee.fit_predict(X_train)# select all rows that are not outliersmask = yhat != -1X_train, y_train = X_train[mask, :], y_train[mask]# summarize the shape of the updated training datasetprint(X_train.shape, y_train.shape)# fit the modelmodel = LinearRegression()model.fit(X_train, y_train)# evaluate the modelyhat = model.predict(X_test)# evaluate predictionsmae = mean_absolute_error(y_test, yhat)print('MAE: %.3f' % mae) Running the example fits and evaluates the model, then reports the MAE. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that only three outliers were identified and removed and the model achieved a MAE of about 3.431, which is not better than the baseline model that achieved 3.417. Perhaps better performance can be achieved with more tuning. (339, 13) (339,)(336, 13) (336,)MAE: 3.431 Further Reading This section provides more resources on the topic if you are looking to go deeper. Related Tutorials One-Class Classification Algorithms for Imbalanced Datasets How to Remove Outliers for Machine Learning Papers Isolation Forest, 2008. Minimum Covariance Determinant and Extensions, 2017. LOF: Identifying Density-based Local Outliers, 2000. Estimating the Support of a High-Dimensional Distribution, 2001. APIs Novelty and Outlier Detection, scikit-learn user guide. sklearn.covariance.EllipticEnvelope API. sklearn.svm.OneClassSVM API. sklearn.neighbors.LocalOutlierFactor API. sklearn.ensemble.IsolationForest API. Summary In this tutorial, you discovered how to use automatic outlier detection and removal to improve machine learning predictive modeling performance. Specifically, you learned: Automatic outlier detection models provide an alternative to statistical techniques with a larger number of input variables with complex and unknown inter-relationships. How to correctly apply automatic outlier detection and removal to the training dataset only to avoid data leakage. How to evaluate and compare predictive modeling pipelines with outliers removed from the training dataset. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Get a Handle on Modern Data Preparation! Prepare Your Machine Learning Data in Minutes …with just a few lines of python code Discover how in my new Ebook: Data Preparation for Machine Learning It provides self-study tutorials with full working code on: Feature Selection, RFE, Data Cleaning, Data Transforms, Scaling, Dimensionality Reduction, and much more… Bring Modern Data Preparation Techniques to Your Machine Learning Projects See What’s Inside"
  },"/Materials-Research-Notes/Chap2-ML/Pytorch_tricks/": {
    "title": "Pytorch tricks",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap2-ML/Pytorch_tricks/",
    "body": "Tensors Functions Constraint output def forward(self,x): x = self.main(x) output = x.clamp(-5,5).torch.sigmoid()*scale return output Models Loss functions Activation functions"
  },"/Materials-Research-Notes/Chap2-ML/SHAP/": {
    "title": "SHAP",
    "keywords": "Lab Notes",
    "url": "/Materials-Research-Notes/Chap2-ML/SHAP/",
    "body": "Contents cliped from the book Interpretable Machine Learning - A Guide for Making Black Box Models Explainable View the original book chapter here: christophm.github.io Shapley Values A prediction can be explained by assuming that each feature value of the instance is a “player” in a game where the prediction is the payout. Shapley values – a method from coalitional game theory – tells us how to fairly distribute the “payout” among the features. General Idea Assume the following scenario: You have trained a machine learning model to predict apartment prices. For a certain apartment it predicts €300,000 and you need to explain this prediction. The apartment has an area of 50 m$^2$, is located on the 2nd floor, has a park nearby and cats are banned: FIGURE 9.17: The predicted price for a 50 m$^2$ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction. The average prediction for all apartments is €310,000. How much has each feature value contributed to the prediction compared to the average prediction? The answer is simple for linear regression models: the effect of each feature is the weight of the feature times the feature value, because of the linearity. For more complex models, we need a different solution. For example, LIME suggests local models to estimate effects. Another solution comes from cooperative game theory: The Shapley value, coined by Shapley (1953)63, is a method for assigning payouts to players depending on their contribution to the total payout. Players cooperate in a coalition and receive a certain profit from this cooperation. Players? Game? Payout? What is the connection to machine learning predictions and interpretability? The “game” is the prediction task for a single instance of the dataset. The “gain” is the actual prediction for this instance minus the average prediction for all instances. The “players” are the feature values of the instance that collaborate to receive the gain (= predict a certain value). In our apartment example, the feature values park-nearby, cat-banned, area-50 and floor-2nd worked together to achieve the prediction of €300,000. Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000): a difference of -€10,000. The answer could be: The park-nearby contributed €30,000; area-50 contributed €10,000; floor-2nd contributed €0; cat-banned contributed -€50,000. The contributions add up to -€10,000, the final prediction minus the average predicted apartment price. How do we calculate the Shapley value for one feature? The Shapley value is the average marginal contribution of a feature value across all possible coalitions. All clear now? In the following figure we evaluate the contribution of the cat-banned feature value when it is added to a coalition of park-nearby and area-50. We simulate that only park-nearby, cat-banned and area-50 are in a coalition by randomly drawing another apartment from the data and using its value for the floor feature. The value floor-2nd was replaced by the randomly drawn floor-1st. Then we predict the price of the apartment with this combination (€310,000). In a second step, we remove cat-banned from the coalition by replacing it with a random value of the cat allowed/banned feature from the randomly drawn apartment. In the example it was cat-allowed, but it could have been cat-banned again. We predict the apartment price for the coalition of park-nearby and area-50 (€320,000). The contribution of cat-banned was €310,000 - €320,000 = -€10,000. This estimate depends on the values of the randomly drawn apartment that served as a “donor” for the cat and floor feature values. We will get better estimates if we repeat this sampling step and average the contributions. FIGURE 9.18: One sample repetition to estimate the contribution of cat-banned to the prediction when added to the coalition of park-nearby and area-50. We repeat this computation for all possible coalitions. The Shapley value is the average of all the marginal contributions to all possible coalitions. The computation time increases exponentially with the number of features. One solution to keep the computation time manageable is to compute contributions for only a few samples of the possible coalitions. The following figure shows all coalitions of feature values that are needed to determine the Shapley value for cat-banned. The first row shows the coalition without any feature values. The second, third and fourth rows show different coalitions with increasing coalition size, separated by “|”. All in all, the following coalitions are possible: No feature values park-nearby area-50 floor-2nd park-nearby+area-50 park-nearby+floor-2nd area-50+floor-2nd park-nearby+area-50+floor-2nd. For each of these coalitions we compute the predicted apartment price with and without the feature value cat-banned and take the difference to get the marginal contribution. The Shapley value is the (weighted) average of marginal contributions. We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model. FIGURE 9.19: All 8 coalitions needed for computing the exact Shapley value of the cat-banned feature value. If we estimate the Shapley values for all feature values, we get the complete distribution of the prediction (minus the average) among the feature values. Examples and Interpretation The interpretation of the Shapley value for feature value $j$ is: The value of the $j$-th feature contributed $\\phi_j$ to the prediction of this particular instance compared to the average prediction for the dataset. The Shapley value works for both classification (if we are dealing with probabilities) and regression. We use the Shapley value to analyze the predictions of a random forest model predicting cervical cancer: FIGURE 9.20: Shapley values for a woman in the cervical cancer dataset. With a prediction of 0.57, this woman’s cancer probability is 0.54 above the average prediction of 0.03. The number of diagnosed STDs increased the probability the most. The sum of contributions yields the difference between actual and average prediction (0.54). For the bike rental dataset, we also train a random forest to predict the number of rented bikes for a day, given weather and calendar information. The explanations created for the random forest prediction of a particular day: FIGURE 9.21: Shapley values for day 285. With a predicted 2409 rental bikes, this day is -2108 below the average prediction of 4518. The weather situation and humidity had the largest negative contributions. The temperature on this day had a positive contribution. The sum of Shapley values yields the difference of actual and average prediction (-2108). Be careful to interpret the Shapley value correctly: The Shapley value is the average contribution of a feature value to the prediction in different coalitions. The Shapley value is NOT the difference in prediction when we would remove the feature from the model. The Shapley Value in Detail This section goes deeper into the definition and computation of the Shapley value for the curious reader. Skip this section and go directly to “Advantages and Disadvantages” if you are not interested in the technical details. We are interested in how each feature affects the prediction of a data point. In a linear model it is easy to calculate the individual effects. Here is what a linear model prediction looks like for one data instance: \\[\\hat{f}(x)=\\beta_0+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] where $x$ is the instance for which we want to compute the contributions. Each $x_j$ is a feature value, with j = 1,…,p. The $\\beta_j$ is the weight corresponding to feature j. The contribution $\\phi_j$ of the j-th feature on the prediction $\\hat{f}(x)$ is: \\[\\phi_j(\\hat{f})=\\beta_{j}x_j-E(\\beta_{j}X_{j})=\\beta_{j}x_j-\\beta_{j}E(X_{j})\\] where $E(\\beta_jX_{j})$ is the mean effect estimate for feature j. The contribution is the difference between the feature effect minus the average effect. Nice! Now we know how much each feature contributed to the prediction. If we sum all the feature contributions for one instance, the result is the following: \\[\\begin{align*}\\sum_{j=1}^{p}\\phi_j(\\hat{f})=&amp;\\sum_{j=1}^p(\\beta_{j}x_j-E(\\beta_{j}X_{j}))\\\\=&amp;(\\beta_0+\\sum_{j=1}^p\\beta_{j}x_j)-(\\beta_0+\\sum_{j=1}^{p}E(\\beta_{j}X_{j}))\\\\=&amp;\\hat{f}(x)-E(\\hat{f}(X))\\end{align*}\\] This is the predicted value for the data point x minus the average predicted value. Feature contributions can be negative. Can we do the same for any type of model? It would be great to have this as a model-agnostic tool. Since we usually do not have similar weights in other model types, we need a different solution. Help comes from unexpected places: cooperative game theory. The Shapley value is a solution for computing feature contributions for single predictions for any machine learning model. The Shapley Value The Shapley value is defined via a value function $val$ of players in S. The Shapley value of a feature value is its contribution to the payout, weighted and summed over all possible feature value combinations: \\[\\phi_j(val)=\\sum_{S\\subseteq\\{1,\\ldots,p\\} \\backslash \\{j\\}}\\frac{|S|!\\left(p-|S|-1\\right)!}{p!}\\left(val\\left(S\\cup\\{j\\}\\right)-val(S)\\right)\\] where S is a subset of the features used in the model, x is the vector of feature values of the instance to be explained and p the number of features. $val_x(S)$ is the prediction for feature values in set S that are marginalized over features that are not included in set S: \\[val_{x}(S)=\\int\\hat{f}(x_{1},\\ldots,x_{p})d\\mathbb{P}_{x\\notin{}S}-E_X(\\hat{f}(X))\\] You actually perform multiple integrations for each feature that is not contained S. A concrete example: The machine learning model works with 4 features x1, x2, x3 and x4 and we evaluate the prediction for the coalition S consisting of feature values x1 and x3: \\[val_{x}(S)=val_{x}(\\{1,3\\})=\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\hat{f}(x_{1},X_{2},x_{3},X_{4})d\\mathbb{P}_{X_2X_4}-E_X(\\hat{f}(X))\\] This looks similar to the feature contributions in the linear model! Do not get confused by the many uses of the word “value”: The feature value is the numerical or categorical value of a feature and instance; the Shapley value is the feature contribution to the prediction; the value function is the payout function for coalitions of players (feature values). The Shapley value is the only attribution method that satisfies the properties Efficiency, Symmetry, Dummy and Additivity, which together can be considered a definition of a fair payout. Efficiency The feature contributions must add up to the difference of prediction for x and the average. \\[\\sum\\nolimits_{j=1}^p\\phi_j=\\hat{f}(x)-E_X(\\hat{f}(X))\\] Symmetry The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions. If \\[val(S \\cup \\{j\\})=val(S\\cup\\{k\\})\\] for all \\[S\\subseteq\\{1,\\ldots, p\\} \\backslash \\{j,k\\}\\] then \\[\\phi_j=\\phi_{k}\\] Dummy A feature j that does not change the predicted value – regardless of which coalition of feature values it is added to – should have a Shapley value of 0. If \\[val(S\\cup\\{j\\})=val(S)\\] for all \\[S\\subseteq\\{1,\\ldots,p\\}\\] then \\[\\phi_j=0\\] Additivity For a game with combined payouts val+val+ the respective Shapley values are as follows: \\[\\phi_j+\\phi_j^{+}\\] Suppose you trained a random forest, which means that the prediction is an average of many decision trees. The Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest. Intuition An intuitive way to understand the Shapley value is the following illustration: The feature values enter a room in random order. All feature values in the room participate in the game (= contribute to the prediction). The Shapley value of a feature value is the average change in the prediction that the coalition already in the room receives when the feature value joins them. Estimating the Shapley Value All possible coalitions (sets) of feature values have to be evaluated with and without the j-th feature to calculate the exact Shapley value. For more than a few features, the exact solution to this problem becomes problematic as the number of possible coalitions exponentially increases as more features are added. Strumbelj et al. (2014)64 propose an approximation with Monte-Carlo sampling: \\[\\hat{\\phi}_{j}=\\frac{1}{M}\\sum_{m=1}^M\\left(\\hat{f}(x^{m}_{+j})-\\hat{f}(x^{m}_{-j})\\right)\\] where $\\hat{f} (x^m_{+j})$ is the prediction for $x$, but with a random number of feature values replaced by feature values from a random data point $z$, except for the respective value of feature $j$. The $x$-vector $x^m_{-j}$ is almost identical to $x^m_{+j}$, but the value $x^m_j$ is also taken from the sampled $z$. Each of these M new instances is a kind of “Frankenstein’s Monster” assembled from two instances. Note that in the following algorithm, the order of features is not actually changed – each feature remains at the same vector position when passed to the predict function. The order is only used as a “trick” here: By giving the features a new order, we get a random mechanism that helps us put together the “Frankenstein’s Monster”. For features that appear left of the feature $x_j$, we take the values from the original observations, and for the features on the right, we take the values from a random instance. Approximate Shapley estimation for single feature value: Output: Shapley value for the value of the j-th feature Required: Number of iterations M, instance of interest x, feature index j, data matrix X, and machine learning model f For all m = 1,…,M: Draw random instance z from the data matrix X Choose a random permutation o of the feature values Order instance x: $x_o=(x_{(1)},\\ldots,x_{(j)},\\ldots,x_{(p)})$ Order instance z: $z_o=(z_{(1)},\\ldots,z_{(j)},\\ldots,z_{(p)})$ Construct two new instances With j: $x_{+j}=(x_{(1)},\\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\\ldots,z_{(p)})$ Without j: $x_{-j}=(x_{(1)},\\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\\ldots,z_{(p)})$ Compute marginal contribution: $\\phi_j^{m}=\\hat{f}(x_{+j})-\\hat{f}(x_{-j})$ Compute Shapley value as the average: $\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}$ First, select an instance of interest x, a feature j and the number of iterations M. For each iteration, a random instance z is selected from the data and a random order of the features is generated. Two new instances are created by combining values from the instance of interest x and the sample z. The instance $x_{+j}$ is the instance of interest, but all values in the order after feature j are replaced by feature values from the sample z. The instance $x_{-j}$ is the same as $x_{+j}$, but in addition has feature j replaced by the value for feature j from the sample z. The difference in the prediction from the black box is computed: \\[\\phi_j^{m}=\\hat{f}(x^m_{+j})-\\hat{f}(x^m_{-j})\\] All these differences are averaged and result in: \\[\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\] Averaging implicitly weighs samples by the probability distribution of X. The procedure has to be repeated for each of the features to get all Shapley values. Advantages The difference between the prediction and the average prediction is fairly distributed among the feature values of the instance – the Efficiency property of Shapley values. This property distinguishes the Shapley value from other methods such as LIME. LIME does not guarantee that the prediction is fairly distributed among the features. The Shapley value might be the only method to deliver a full explanation. In situations where the law requires explainability – like EU’s “right to explanations” – the Shapley value might be the only legally compliant method, because it is based on a solid theory and distributes the effects fairly. I am not a lawyer, so this reflects only my intuition about the requirements. The Shapley value allows contrastive explanations. Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point. This contrastiveness is also something that local models like LIME do not have. The Shapley value is the only explanation method with a solid theory. The axioms – efficiency, symmetry, dummy, additivity – give the explanation a reasonable foundation. Methods like LIME assume linear behavior of the machine learning model locally, but there is no theory as to why this should work. It is mind-blowing to explain a prediction as a game played by the feature values. Disadvantages The Shapley value requires a lot of computing time. In 99.9% of real-world problems, only the approximate solution is feasible. An exact computation of the Shapley value is computationally expensive because there are 2k possible coalitions of the feature values and the “absence” of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation. The exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M. Decreasing M reduces computation time, but increases the variance of the Shapley value. There is no good rule of thumb for the number of iterations M. M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time. It should be possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for Shapley values for machine learning predictions. The Shapley value can be misinterpreted. The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training. The interpretation of the Shapley value is: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value. The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features). Explanations created with the Shapley value method always use all the features. Humans prefer selective explanations, such as those produced by LIME. LIME might be the better choice for explanations lay-persons have to deal with. Another solution is SHAP introduced by Lundberg and Lee (2016)65, which is based on the Shapley value, but can also provide explanations with few features. The Shapley value returns a simple value per feature, but no prediction model like LIME. This means it cannot be used to make statements about changes in prediction for changes in the input, such as: “If I were to earn €300 more a year, my credit score would increase by 5 points.” Another disadvantage is that you need access to the data if you want to calculate the Shapley value for a new data instance. It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data. This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data. Like many other permutation-based interpretation methods, the Shapley value method suffers from inclusion of unrealistic data instances when features are correlated. To simulate that a feature value is missing from a coalition, we marginalize the feature. This is achieved by sampling values from the feature’s marginal distribution. This is fine as long as the features are independent. When features are dependent, then we might sample feature values that do not make sense for this instance. But we would use those to compute the feature’s Shapley value. One solution might be to permute correlated features together and get one mutual Shapley value for them. Another adaptation is conditional sampling: Features are sampled conditional on the features that are already in the team. While conditional sampling fixes the issue of unrealistic data points, a new issue is introduced: The resulting values are no longer the Shapley values to our game, since they violate the symmetry axiom, as found out by Sundararajan et al. (2019)66 and further discussed by Janzing et al. (2020)67. Software and Alternatives Shapley values are implemented in both the iml and fastshap packages for R. In Julia, you can use Shapley.jl. SHAP, an alternative estimation method for Shapley values, is presented in the next chapter. Another approach is called breakDown, which is implemented in the breakDown R package68. BreakDown also shows the contributions of each feature to the prediction, but computes them step by step. Let us reuse the game analogy: We start with an empty team, add the feature value that would contribute the most to the prediction and iterate until all feature values are added. How much each feature value contributes depends on the respective feature values that are already in the “team”, which is the big drawback of the breakDown method. It is faster than the Shapley value method, and for models without interactions, the results are the same. SHAP (SHapley Additive Explanations) Contents cliped from the book Interpretable Machine Learning - A Guide for Making Black Box Models Explainable View the original book chapter here: christophm.github.io SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2017)69 is a method to explain individual predictions. SHAP is based on the game theoretically optimal Shapley values. There are two reasons why SHAP got its own chapter and is not a subchapter of Shapley values. First, the SHAP authors proposed KernelSHAP, an alternative, kernel-based estimation approach for Shapley values inspired by local surrogate models. And they proposed TreeSHAP, an efficient estimation approach for tree-based models. Second, SHAP comes with many global interpretation methods based on aggregations of Shapley values. This chapter explains both the new estimation approaches and the global interpretation methods. Just looking for the correct interpretation of SHAP lots? Save yourself time and get the SHAP plots cheat sheet. I recommend reading the chapters on Shapley values and local models (LIME) first. Definition The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the “payout” (= the prediction) among the features. A player can be an individual feature value, e.g. for tabular data. A player can also be a group of feature values. For example to explain an image, pixels can be grouped to superpixels and the prediction distributed among them. One innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feature attribution method, a linear model. That view connects LIME and Shapley values. SHAP specifies the explanation as: \\[g(z')=\\phi_0+\\sum_{j=1}^M\\phi_jz_j'\\] where g is the explanation model, $z’\\in{0,1}^M$ is the coalition vector, M is the maximum coalition size and $\\phi_j\\in\\mathbb{R}$ is the feature attribution for a feature j, the Shapley values. What I call “coalition vector” is called “simplified features” in the SHAP paper. I think this name was chosen, because for e.g. image data, the images are not represented on the pixel level, but aggregated to superpixels. I believe it is helpful to think about the z’s as describing coalitions: In the coalition vector, an entry of 1 means that the corresponding feature value is “present” and 0 that it is “absent”. This should sound familiar to you if you know about Shapley values. To compute Shapley values, we simulate that only some feature values are playing (“present”) and some are not (“absent”). The representation as a linear model of coalitions is a trick for the computation of the $\\phi$’s. For x, the instance of interest, the coalition vector x’ is a vector of all 1’s, i.e. all feature values are “present”. The formula simplifies to: \\[g(x')=\\phi_0+\\sum_{j=1}^M\\phi_j\\] You can find this formula in similar notation in the Shapley value chapter. More about the actual estimation comes later. Let us first talk about the properties of the $\\phi$’s before we go into the details of their estimation. Shapley values are the only solution that satisfies properties of Efficiency, Symmetry, Dummy and Additivity. SHAP also satisfies these, since it computes Shapley values. In the SHAP paper, you will find discrepancies between SHAP properties and Shapley properties. SHAP describes the following three desirable properties: 1) Local accuracy \\[\\hat{f}(x)=g(x')=\\phi_0+\\sum_{j=1}^M\\phi_jx_j'\\] If you define $\\phi_0=E_X(\\hat{f}(x))$ and set all $x_j’$ to 1, this is the Shapley efficiency property. Only with a different name and using the coalition vector. \\[\\hat{f}(x)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j'=E_X(\\hat{f}(X))+\\sum_{j=1}^M\\phi_j\\] 2) Missingness \\[x_j'=0\\Rightarrow\\phi_j=0\\] Missingness says that a missing feature gets an attribution of zero. Note that $x_j’$ refers to the coalitions where a value of 0 represents the absence of a feature value. In coalition notation, all feature values $x_j’$ of the instance to be explained should be ‘1’. The presence of a 0 would mean that the feature value is missing for the instance of interest. This property is not among the properties of the “normal” Shapley values. So why do we need it for SHAP? Lundberg calls it a “minor book-keeping property”. A missing feature could – in theory – have an arbitrary Shapley value without hurting the local accuracy property, since it is multiplied with $x_j’=0$. The Missingness property enforces that missing features get a Shapley value of 0. In practice, this is only relevant for features that are constant. 3) Consistency Let $\\hat{f} x (z’) = \\hat{f} (h_x (z’))$, and $z{-j}’$ indicate that $z_j’=0$. For any two models f and f’ that satisfy: \\[\\hat{f}_x'(z')-\\hat{f}_x'(z_{-j}')\\geq{}\\hat{f}_x(z')-\\hat{f}_x(z_{-j}')\\] for all inputs $z’\\in{0,1}^M$, then: \\[\\phi_j(\\hat{f}',x)\\geq\\phi_j(\\hat{f},x)\\] The consistency property says that if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same. From Consistency the Shapley properties Linearity, Dummy and Symmetry follow, as described in the Appendix of Lundberg and Lee. KernelSHAP KernelSHAP estimates for an instance x the contributions of each feature value to the prediction. KernelSHAP consists of five steps: Sample coalitions $z_k’\\in{0,1}^M,\\quad{}k\\in{1,\\ldots,K}$ (1 = feature present in coalition, 0 = feature absent). Get prediction for each $z_k’$ by first converting $z_k’$ to the original feature space and then applying model $\\hat{f}: \\hat{f}(h_x(z_k’))$ Compute the weight for each $z_k’$ with the SHAP kernel. Fit weighted linear model. Return Shapley values $\\phi_k$, the coefficients from the linear model. We can create a random coalition by repeated coin flips until we have a chain of 0’s and 1’s. For example, the vector of (1,0,1,0) means that we have a coalition of the first and third features. The K sampled coalitions become the dataset for the regression model. The target for the regression model is the prediction for a coalition. (“Hold on!,” you say. “The model has not been trained on these binary coalition data and cannot make predictions for them.”) To get from coalitions of feature values to valid data instances, we need a function $h_x(z’)=z$ where $h_x:{0,1}^M\\rightarrow\\mathbb{R}^p$. The function $h_x$ maps 1’s to the corresponding value from the instance x that we want to explain. For tabular data, it maps 0’s to the values of another instance that we sample from the data. This means that we equate “feature value is absent” with “feature value is replaced by random feature value from data”. For tabular data, the following figure visualizes the mapping from coalitions to feature values: FIGURE 9.22: Function $h_x$ maps a coalition to a valid instance. For present features (1), $h_x$ maps to the feature values of x. For absent features (0), $h_x$ maps to the values of a randomly sampled data instance. $h_x$ for tabular data treats feature $X_j$ and $X_{-j}$ (the other features) as independent and integrates over the marginal distribution: \\[\\hat{f}(h_x(z'))=E_{X_{-j}}[\\hat{f}(x)]\\] Sampling from the marginal distribution means ignoring the dependence structure between present and absent features. KernelSHAP therefore suffers from the same problem as all permutation-based interpretation methods. The estimation puts too much weight on unlikely instances. Results can become unreliable. But it is necessary to sample from the marginal distribution. The solution would be to sample from the conditional distribution, which changes the value function, and therefore the game to which Shapley values are the solution. As a result, the Shapley values have a different interpretation: For example, a feature that might not have been used by the model at all can have a non-zero Shapley value when the conditional sampling is used. For the marginal game, this feature value would always get a Shapley value of 0, because otherwise it would violate the Dummy axiom. For images, the following figure describes a possible mapping function: FIGURE 9.23: Function $h_x$ maps coalitions of superpixels (sp) to images. Superpixels are groups of pixels. For present features (1), $h_x$ returns the corresponding part of the original image. For absent features (0), $h_x$ greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option. The big difference to LIME is the weighting of the instances in the regression model. LIME weights the instances according to how close they are to the original instance. The more 0’s in the coalition vector, the smaller the weight in LIME. SHAP weights the sampled instances according to the weight the coalition would get in the Shapley value estimation. Small coalitions (few 1’s) and large coalitions (i.e. many 1’s) get the largest weights. The intuition behind it is: We learn most about individual features if we can study their effects in isolation. If a coalition consists of a single feature, we can learn about this feature’s isolated main effect on the prediction. If a coalition consists of all but one feature, we can learn about this feature’s total effect (main effect plus feature interactions). If a coalition consists of half the features, we learn little about an individual feature’s contribution, as there are many possible coalitions with half of the features. To achieve Shapley compliant weighting, Lundberg et al. propose the SHAP kernel: \\[\\pi_{x}(z')=\\frac{(M-1)}{\\binom{M}{|z'|}|z'|(M-|z'|)}\\] Here, M is the maximum coalition size and $|z’|$ the number of present features in instance z’. Lundberg and Lee show that linear regression with this kernel weight yields Shapley values. If you would use the SHAP kernel with LIME on the coalition data, LIME would also estimate Shapley values! We can be a bit smarter about the sampling of coalitions: The smallest and largest coalitions take up most of the weight. We get better Shapley value estimates by using some of the sampling budget K to include these high-weight coalitions instead of sampling blindly. We start with all possible coalitions with 1 and M-1 features, which makes 2 times M coalitions in total. When we have enough budget left (current budget is K - 2M), we can include coalitions with 2 features and with M-2 features and so on. From the remaining coalition sizes, we sample with readjusted weights. We have the data, the target and the weights; Everything we need to build our weighted linear regression model: \\[g(z')=\\phi_0+\\sum_{j=1}^M\\phi_jz_j'\\] We train the linear model g by optimizing the following loss function L: \\[L(\\hat{f},g,\\pi_{x})=\\sum_{z'\\in{}Z}[\\hat{f}(h_x(z'))-g(z')]^2\\pi_{x}(z')\\] where Z is the training data. This is the good old boring sum of squared errors that we usually optimize for linear models. The estimated coefficients of the model, the $\\phi_j$’s, are the Shapley values. Since we are in a linear regression setting, we can also make use of the standard tools for regression. For example, we can add regularization terms to make the model sparse. If we add an L1 penalty to the loss L, we can create sparse explanations. (I am not so sure whether the resulting coefficients would still be valid Shapley values though.) TreeSHAP Lundberg et al. (2018)70 proposed TreeSHAP, a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees. TreeSHAP was introduced as a fast, model-specific alternative to KernelSHAP, but it turned out that it can produce unintuitive feature attributions. TreeSHAP defines the value function using the conditional expectation $E_{X_j | X_{-j}}(\\hat{f}(x)|x_j)$ instead of the marginal expectation. The problem with the conditional expectation is that features that have no influence on the prediction function f can get a TreeSHAP estimate different from zero as shown by Sundararajan et al. (2019) 71 and Janzing et al. (2019) 72. The non-zero estimate can happen when the feature is correlated with another feature that actually has an influence on the prediction. How much faster is TreeSHAP? Compared to exact KernelSHAP, it reduces the computational complexity from $O(TL2^M)$ to $O(TLD^2)$, where T is the number of trees, L is the maximum number of leaves in any tree and D the maximal depth of any tree. TreeSHAP uses the conditional expectation $E_{X_j | X_{-j}}(\\hat{f}(x)|x_j)$ to estimate effects. I will give you some intuition on how we can compute the expected prediction for a single tree, an instance x and feature subset S. If we conditioned on all features – if S was the set of all features – then the prediction from the node in which the instance x falls would be the expected prediction. If we would not condition the prediction on any feature – if S was empty – we would use the weighted average of predictions of all terminal nodes. If S contains some, but not all, features, we ignore predictions of unreachable nodes. Unreachable means that the decision path that leads to this node contradicts values in xS$x_S$. From the remaining terminal nodes, we average the predictions weighted by node sizes (i.e. number of training samples in that node). The mean of the remaining terminal nodes, weighted by the number of instances per node, is the expected prediction for x given S. The problem is that we have to apply this procedure for each possible subset S of the feature values. TreeSHAP computes in polynomial time instead of exponential. The basic idea is to push all possible subsets S down the tree at the same time. For each decision node we have to keep track of the number of subsets. This depends on the subsets in the parent node and the split feature. For example, when the first split in a tree is on feature x3, then all the subsets that contain feature x3 will go to one node (the one where x goes). Subsets that do not contain feature x3 go to both nodes with reduced weight. Unfortunately, subsets of different sizes have different weights. The algorithm has to keep track of the overall weight of the subsets in each node. This complicates the algorithm. I refer to the original paper for details of TreeSHAP. The computation can be expanded to more trees: Thanks to the Additivity property of Shapley values, the Shapley values of a tree ensemble is the (weighted) average of the Shapley values of the individual trees. Next, we will look at SHAP explanations in action. Examples I trained a random forest classifier with 100 trees to predict the risk for cervical cancer. We will use SHAP to explain individual predictions. We can use the fast TreeSHAP estimation method instead of the slower KernelSHAP method, since a random forest is an ensemble of trees. But instead of relying on the conditional distribution, this example uses the marginal distribution. This is described in the package, but not in the original paper. The Python TreeSHAP function is slower with the marginal distribution, but still faster than KernelSHAP, since it scales linearly with the rows in the data. Because we use the marginal distribution here, the interpretation is the same as in the Shapley value chapter. But with the Python shap package comes a different visualization: You can visualize feature attributions such as Shapley values as “forces”. Each feature value is a force that either increases or decreases the prediction. The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions. In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance. The following figure shows SHAP explanation force plots for two women from the cervical cancer dataset: FIGURE 9.24: SHAP values to explain the predicted cancer probabilities of two individuals. The baseline – the average predicted probability – is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk. These were explanations for individual predictions. Shapley values can be combined into global explanations. If we run SHAP for every instance, we get a matrix of Shapley values. This matrix has one row per data instance and one column per feature. We can interpret the entire model by analyzing the Shapley values in this matrix. We start with SHAP feature importance. SHAP Feature Importance The idea behind SHAP feature importance is simple: Features with large absolute Shapley values are important. Since we want the global importance, we average the absolute Shapley values per feature across the data: \\[I_j=\\frac{1}{n}\\sum_{i=1}^n{}|\\phi_j^{(i)}|\\] Next, we sort the features by decreasing importance and plot them. The following figure shows the SHAP feature importance for the random forest trained before for predicting cervical cancer. FIGURE 9.25: SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis). SHAP feature importance is an alternative to permutation feature importance. There is a big difference between both importance measures: Permutation feature importance is based on the decrease in model performance. SHAP is based on magnitude of feature attributions. The feature importance plot is useful, but contains no information beyond the importances. For a more informative plot, we will next look at the summary plot. SHAP Summary Plot The summary plot combines feature importance with feature effects. Each point on the summary plot is a Shapley value for a feature and an instance. The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. The color represents the value of the feature from low to high. Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature. The features are ordered according to their importance. FIGURE 9.26: SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world. In the summary plot, we see first indications of the relationship between the value of a feature and the impact on the prediction. But to see the exact form of the relationship, we have to look at SHAP dependence plots. SHAP Dependence Plot SHAP feature dependence might be the simplest global interpretation plot: 1) Pick a feature. 2) For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis. 3) Done. Mathematically, the plot contains the following points: ${(x_j^{(i)},\\phi_j^{(i)})}_{i=1}^n$ The following figure shows the SHAP feature dependence for years on hormonal contraceptives: FIGURE 9.27: SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability. SHAP dependence plots are an alternative to partial dependence plots and accumulated local effects. While PDP and ALE plot show average effects, SHAP dependence also shows the variance on the y-axis. Especially in case of interactions, the SHAP dependence plot will be much more dispersed in the y-axis. The dependence plot can be improved by highlighting these feature interactions. SHAP Interaction Values The interaction effect is the additional combined feature effect after accounting for the individual feature effects. The Shapley interaction index from game theory is defined as: \\[\\phi_{i,j}=\\sum_{S\\subseteq\\backslash\\{i,j\\}}\\frac{|S|!(M-|S|-2)!}{2(M-1)!}\\delta_{ij}(S)\\] when $i\\neq{}j$ and: \\[\\delta_{ij}(S)=\\hat{f}_x(S\\cup\\{i,j\\})-\\hat{f}_x(S\\cup\\{i\\})-\\hat{f}_x(S\\cup\\{j\\})+\\hat{f}_x(S)\\] This formula subtracts the main effect of the features so that we get the pure interaction effect after accounting for the individual effects. We average the values over all possible feature coalitions S, as in the Shapley value computation. When we compute SHAP interaction values for all features, we get one matrix per instance with dimensions M x M, where M is the number of features. How can we use the interaction index? For example, to automatically color the SHAP feature dependence plot with the strongest interaction: FIGURE 9.28: SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of a STD increases the predicted cancer risk. For more years on contraceptives, the occurence of a STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits). Clustering Shapley Values You can cluster your data with the help of Shapley values. The goal of clustering is to find groups of similar instances. Normally, clustering is based on features. Features are often on different scales. For example, height might be measured in meters, color intensity from 0 to 100 and some sensor output between -1 and 1. The difficulty is to compute distances between instances with such different, non-comparable features. SHAP clustering works by clustering the Shapley values of each instance. This means that you cluster instances by explanation similarity. All SHAP values have the same unit – the unit of the prediction space. You can use any clustering method. The following example uses hierarchical agglomerative clustering to order the instances. The plot consists of many force plots, each of which explains the prediction of an instance. We rotate the force plots vertically and place them side by side according to their clustering similarity. FIGURE 9.29: Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. One cluster stands out: On the right is a group with a high predicted cancer risk. Advantages Since SHAP computes Shapley values, all the advantages of Shapley values apply: SHAP has a solid theoretical foundation in game theory. The prediction is fairly distributed among the feature values. We get contrastive explanations that compare the prediction with the average prediction. SHAP connects LIME and Shapley values. This is very useful to better understand both methods. It also helps to unify the field of interpretable machine learning. SHAP has a fast implementation for tree-based models. I believe this was key to the popularity of SHAP, because the biggest barrier for adoption of Shapley values is the slow computation. The fast computation makes it possible to compute the many Shapley values needed for the global model interpretations. The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots. With SHAP, global interpretations are consistent with the local explanations, since the Shapley values are the “atomic unit” of the global interpretations. If you use LIME for local explanations and partial dependence plots plus permutation feature importance for global explanations, you lack a common foundation. Disadvantages KernelSHAP is slow. This makes KernelSHAP impractical to use when you want to compute Shapley values for many instances. Also all global SHAP methods such as SHAP feature importance require computing Shapley values for a lot of instances. KernelSHAP ignores feature dependence. Most other permutation based interpretation methods have this problem. By replacing feature values with values from random instances, it is usually easier to randomly sample from the marginal distribution. However, if features are dependent, e.g. correlated, this leads to putting too much weight on unlikely data points. TreeSHAP solves this problem by explicitly modeling the conditional expected prediction. TreeSHAP can produce unintuitive feature attributions. While TreeSHAP solves the problem of extrapolating to unlikely data points, it does so by changing the value function and therefore slightly changes the game. TreeSHAP changes the value function by relying on the conditional expected prediction. With the change in the value function, features that have no influence on the prediction can get a TreeSHAP value different from zero. The disadvantages of Shapley values also apply to SHAP: Shapley values can be misinterpreted and access to data is needed to compute them for new data (except for TreeSHAP). It is possible to create intentionally misleading interpretations with SHAP, which can hide biases 73. If you are the data scientist creating the explanations, this is not an actual problem (it would even be an advantage if you are the evil data scientist who wants to create misleading explanations). For the receivers of a SHAP explanation, it is a disadvantage: they cannot be sure about the truthfulness of the explanation. Software The authors implemented SHAP in the shap Python package. This implementation works for tree-based models in the scikit-learn machine learning library for Python. The shap package was also used for the examples in this chapter. SHAP is integrated into the tree boosting frameworks xgboost and LightGBM. In R, there are the shapper and fastshap packages. SHAP is also included in the R xgboost package."
  },"/Materials-Research-Notes/Chap2-ML/committee_models/": {
    "title": "Committee neural network potentials",
    "keywords": "Lab Notes",
    "url": "/Materials-Research-Notes/Chap2-ML/committee_models/",
    "body": "Concept Instead of a single model, multiple models are trained independently to form a committee that offers: Averaging over the predictions improves accuracy Disagreement between the committee (measured by the standard deviation of the predictions of the members), provides access to an estimate of the generalisation error Substantially reduce overfitting issues Active learning strategy known as query by committee (QbC): adding previously unlabeled data with maximal committee disagreement to the training set to systematically improve the model"
  },"/Materials-Research-Notes/Chap2-ML/machine_learned_potential/": {
    "title": "Machine learned potentials",
    "keywords": "Lab Notes",
    "url": "/Materials-Research-Notes/Chap2-ML/machine_learned_potential/",
    "body": "First generation: 2007 Behler and Parrinello[ 1] Encode the local (within a certain cutoff radius) chemical environment of each atom in a descriptor, e.g., using symmetry functions[ 2] as input to predict atomic energies. Total potential energy is modeled as the sum of individual contributions, and forces are obtained by derivation of potential energy with respect to atomic positions. Drawback:"
  },"/Materials-Research-Notes/Chap3-MatNotes/2023-05-12-Comp_Mat_Concep/": {
    "title": "Concepts and methods in computational materials science",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap3-MatNotes/2023-05-12-Comp_Mat_Concep/",
    "body": "Author: Lei Lei Date: 28-03-2022 Stability of structure Formation energy Energy above convex hull Convex hull is a plot of formation energy with respect to the composition which connects phases that are lower in energy than any other phases or a linear combination of phases at that composition. Phases that lie on the convex hull are thermodynamically stable and the ones above it are either metastable or unstable. This plot can only give an idea about the stability of the structure at 0 K. Higher temperature calculation (Phonopy is one of the codes) can assure the stability of the given structure at the working temperature (mostly greater than 0 K). The convex hull (represented by the lines at the bottom of the phase diagram) represents the set of lowest possible potential energy states you can get from either single materials (at the black points) or mixtures of those materials (along the black lines connecting the points). If a compound is above a line in the convex hull (e.g. the lighter blue dots in the below image), it will spontaneously decompose into products on the endpoints of that line. The “energy_above_hull” for a given material is the distance from its dot in the below image and the lines connecting the black dots at the bottom. Preparation Thermometer Sand bath Solution preparation Chemical quantities         CH3NH3PbI3 PbI2 CH3NH3I GBL Quantity 6.915 g 2.385 g 10 mL CH3NH3PbBr3 PbBr2 CH3NH3Br DMF Quantity 4.5876 g 1.3996 g 10 mL The actual quantities of chemicals used are shown in Fig.1. Fig.1 The setup of crystal growth apparatus using sand bath within the glovebox. Temperatures GBL has a maximal solubility for MAPbI3 precursors at 60 °C1, and the solution is prepared and filtered under 60 °C. DMF has a maximal solubility for MAPbBr3 precursors at room temperature. Filtration The precursor solutions were filtered with a 0.22-0.25 μm PVDF syringe filter (Fig.2). Fig.2 Photograph of the syringe filter used. Crystal growth Apparatus The crystal growth used the retrograde solubility of MAPbX3 precursors as reported in Ref1. The apparatus setup is shown in Fig.3 which used sand bath instead of oil bath. Fig.3 The setup of crystal growth apparatus using sand bath within the glovebox. Clean vials The vials were cleaned by detergent and deionised water. Fig.4 The cleaned vials to be used for the growth of MAPbX3 crystals. MAPbBr3 growth First trial The temperature of the sand bath was 87.5 °C as shown in Fig.5. Fig.5 The cleaned vials to be used for the growth of MAPbX3 crystals. A MAPBr seed was added to the filtered precursor solution before the growth. The temperature rose to 89 °C at the beginning of crystal growth (19:40). Fig.6 Photograph of vials immersed in the sand bath for the growth of MAPbBr3 crystals from the precursor solution. The temperature was too high for growth, a lot of small crystallites form. The vials were taken out for overnight to allow the dissolve of these crystallites. A lower temperature was set for another trial. The second go Fig.7 Photograph of the crystallites formed due to the fast nuleation. The sand bath temperature was down to 75 °C and the vials containing the precursor solutions were placed into the bath. Fig.8 Photograph of vials immersed in the sand bath (75 °C) for the growth of MAPbBr3 crystals from the precursor solution. The temperature decreased to ~ 72 °C the second morning. For Further growth the temperature was increased to 82 °C. MAPbI3 growth"
  },"/Materials-Research-Notes/Chap4-HSTORE/2022-03-29-AIMD-simulation-with-efield/": {
    "title": "AIMD simulation for MAPbI3 under electric filed",
    "keywords": "Lab Notes",
    "url": "/Materials-Research-Notes/Chap4-HSTORE/2022-03-29-AIMD-simulation-with-efield/",
    "body": "Structure optimisation of supercell with vacancies ### MA3ZnI3 structure MA3ZnI3 with different water contents AIMD simulation Start from structure optimisation. You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. Jekyll requires blog post files to be named according to the following format: YEAR-MONTH-DAY-title.MARKUP Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and MARKUP is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for code snippets: def print_hi(name) puts \"Hi, #{name}\" end print_hi('Tom') #=&gt; prints 'Hi, Tom' to STDOUT. Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk."
  },"/Materials-Research-Notes/Chap4-HSTORE/2023-05-12-interstitial-hydrides/": {
    "title": "Solid hydrogen storage materials: interstitial hydrides",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap4-HSTORE/2023-05-12-interstitial-hydrides/",
    "body": "Author: Lei Lei Date: 28-03-2022 Objectives Prepare MAPbX3 crystals for MuSR and impedance measurements using the method from Ref1. Preparation Thermometer Sand bath Solution preparation Chemical quantities         CH3NH3PbI3 PbI2 CH3NH3I GBL Quantity 6.915 g 2.385 g 10 mL CH3NH3PbBr3 PbBr2 CH3NH3Br DMF Quantity 4.5876 g 1.3996 g 10 mL The actual quantities of chemicals used are shown in Fig.1. Fig.1 The setup of crystal growth apparatus using sand bath within the glovebox. Temperatures GBL has a maximal solubility for MAPbI3 precursors at 60 °C1, and the solution is prepared and filtered under 60 °C. DMF has a maximal solubility for MAPbBr3 precursors at room temperature. Filtration The precursor solutions were filtered with a 0.22-0.25 μm PVDF syringe filter (Fig.2). Fig.2 Photograph of the syringe filter used. Crystal growth Apparatus The crystal growth used the retrograde solubility of MAPbX3 precursors as reported in Ref1. The apparatus setup is shown in Fig.3 which used sand bath instead of oil bath. Fig.3 The setup of crystal growth apparatus using sand bath within the glovebox. Clean vials The vials were cleaned by detergent and deionised water. Fig.4 The cleaned vials to be used for the growth of MAPbX3 crystals. MAPbBr3 growth First trial The temperature of the sand bath was 87.5 °C as shown in Fig.5. Fig.5 The cleaned vials to be used for the growth of MAPbX3 crystals. A MAPBr seed was added to the filtered precursor solution before the growth. The temperature rose to 89 °C at the beginning of crystal growth (19:40). Fig.6 Photograph of vials immersed in the sand bath for the growth of MAPbBr3 crystals from the precursor solution. The temperature was too high for growth, a lot of small crystallites form. The vials were taken out for overnight to allow the dissolve of these crystallites. A lower temperature was set for another trial. The second go Fig.7 Photograph of the crystallites formed due to the fast nuleation. The sand bath temperature was down to 75 °C and the vials containing the precursor solutions were placed into the bath. Fig.8 Photograph of vials immersed in the sand bath (75 °C) for the growth of MAPbBr3 crystals from the precursor solution. The temperature decreased to ~ 72 °C the second morning. For Further growth the temperature was increased to 82 °C. MAPbI3 growth"
  },"/Materials-Research-Notes/Chap5-CompMat/VASP-tricks/": {
    "title": "VASP errors and practical tricks",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap5-CompMat/VASP-tricks/",
    "body": "Inconsistent Bravais lattice types found for crystalline and reciprocal lattice Suggested SOLUTIONS Refine the lattice parameters of your structure, and/or try changing SYMPREC SYMPREC: determines to which accuracy the positions in the POSCAR file must be specified. Default 10&minus;5. The error was fixed by changing SYMPREC to $10^{-4}$. If the system is orthorhombic, the following setting maybe helpful: Don’t use ISIF=3 but use ISIF=4 instead! Then the cell shape only (b/a, c/a) will be relaxed at fixed volume and the volume itself can be given in POSCAR on the second line as a negative value (i.e., minus volume). From that and the initial three lattice vectors below VASP calculates internally the correct a, b, c. After relaxation you will find changes in these three lattice vectors (lenghts) but such that volume will remain conserved. At the end, on CONTCAR you can then find as usual a “scaling parameter” in the second line and the three altered lattice vectors from which you can (after multiplication with the “scaling parameter”) determine the (relaxed) lattice constants a, b, and c for the given volume. And of course you will also know the total energy then but even hydrostatic pressure printed in OUTCAR (“pressure = …”). So you could not only determine an energy-volume but even a pressure-volume curve! Only if you have no idea at all about the equilibrium lattice constants / volume you might consider a “pre-relaxation” run with ISIF=3 first before you begin to sample different volumes around the (approximate) equilibrium volume with ISIF=4 … Just take care that your plane-wave cutoff is high enough (in order to avoid too large Pullay stresses – I usually recommend to approximately double the cutoff with respect to what is set by VASP for PREC=high/accurate) and that you perform enough relaxation steps to get all stress tensor components converged to at least better than order of 0.1 kBar. And also use a rather small EDIFF (use maybe values of about 1-9 to 1-6, in order to obtain well-converged wave functions which are important for numerically accurate forces and stresses). And overall try to be as accurate as possible (e.g. PREC=high or PREC=accurate is recommended, maybe also ADDGRID=.TRUE.). This strategy works by the way for all types of (non-cubic) lattices, also monoclinic or triclinic ones (getting not only the three lattice constants but also the one or three angles). And even for tetragonal or hexagonal ones it can still be useful. Fatal error: bracketing interval incorrect Error message: ZBRENT: fatal error: bracketing interval incorrect please rerun with smaller EDIFF, or copy CONTCAR to POSCAR and continue Suggested SOLUTIONS: Have a look at the OUTCAR file, at the forces of the last ionic step before Brent's algorithm fails if the calculation is very well-converged already, please proceed using IBRION=1; ADDGRID=.TRUE. and a larger ENCUT if this error appears at an early stage of the relaxation, pelase check if your input geometry was reasonable, eg by having a look at the interatomic distances (written in OUTCAR) or the forces of the first geometries. It may then help to increase the accuracy (PREC = Accurate and a higher ENCUT) and to decrease POTIM to avoid too large stepwidhts in the first relaxation step."
  },"/Materials-Research-Notes/Chap6-Visualisation/Jupyter/jupyter_tricks/": {
    "title": "Jupyter Notebook Tricks",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap6-Visualisation/Jupyter/jupyter_tricks/",
    "body": "Hotkeys Command mode (press Esc to enable) Enter enter edit mode Shift+­Enter run cell, select below Ctrl+Enter run cell Alt+Enter run cell, insert below Y to code M to markdown R to raw 1 to heading 1 2,3,4,5,6 to heading 2,3,4,5,6 Up/K select cell above Down/J select cell below A/B insert cell above/­below X cut selected cell C copy selected cell Shift+V paste cell above V paste cell below Z undo last cell deletion D delete selected cell Shift+M merge cell below Ctrl+S Save and Checkpoint L toggle line numbers O toggle output Shift+O toggle output scrolling Esc close pager H show keyboard shortcut help dialog I interrupt kernel 0 restart kernel Space scroll down Shift+­Space scroll up Shift ignore Edit Mode (press Enter to enable) Tab code completion or indent Shift+Tab tooltip Ctrl+] indent Ctrl+[ dedent Ctrl+A select all Ctrl+Z undo Ctrl+S­hift+Z redo Ctrl+Y redo Ctrl+Home go to cell start Ctrl+Up go to cell start Ctrl+End go to cell end Ctrl+Down go to cell end Ctrl+Left go one word left Ctrl+Right go one word right Ctrl+B­ack­space delete word before Ctrl+D­elete delete word after Esc command mode Ctrl+M command mode Ctrl+S­hift+minus split cell Ctrl+S Save and Checkpoint Up move cursor up or previous cell Down move cursor down or next cell Ctrl+/ toggle comment on current or selected lines Recover deleted notebooks Retrieve your Ipython history by writting into a file: %history -g -f filename Check (would be the end part if you just deleted it) the file: filename contains all scripts you’ve run."
  },"/Materials-Research-Notes/Chap6-Visualisation/example_codes/": {
    "title": "Matplotlib quick examples",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap6-Visualisation/example_codes/",
    "body": "Bar plots import matplotlib.pyplot as plt import numpy as np x = np.array(['ERS1', 'ERS2', 'ETR1', 'ETR2', 'EIN4']) x_position = np.arange(len(x)) y_control = np.array([12.0, 3.1, 11.8, 2.9, 6.2]) y_stress = np.array([6.2, 3.4, 6.8, 2.0, 6.8]) fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.bar(x_position, y_control, width=0.4, label='control') ax.bar(x_position + 0.4, y_stress, width=0.4, label='stress') ax.legend() ax.set_xticks(x_position + 0.2) ax.set_xticklabels(x) plt.show() Stacked bar plot import matplotlib.pyplot as plt labels = ['G1', 'G2', 'G3', 'G4', 'G5'] men_means = [20, 35, 30, 35, 27] women_means = [25, 32, 34, 20, 25] men_std = [2, 3, 4, 1, 2] women_std = [3, 5, 2, 3, 3] width = 0.35 # the width of the bars: can also be len(x) sequence fig, ax = plt.subplots() ax.bar(labels, men_means, width, yerr=men_std, label='Men') ax.bar(labels, women_means, width, yerr=women_std, bottom=men_means, label='Women') ax.set_ylabel('Scores') ax.set_title('Scores by group and gender') ax.legend() plt.show() Box plot import numpy as np import matplotlib.pyplot as plt # Make a random dataset: height = [3, 12, 5, 18, 45] bars = ('A', 'B', 'C', 'D', 'E') y_pos = np.arange(len(bars)) # Create bars plt.bar(y_pos, height) # Create names on the x-axis plt.xticks(y_pos, bars) # Show graph plt.show()"
  },"/Materials-Research-Notes/Chap6-Visualisation/matplotlib_tricks/": {
    "title": "Matplotlib tricks",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap6-Visualisation/matplotlib_tricks/",
    "body": "Formatting &amp; Styling Set tick font size plt.xticks(fontsize=) ax.set_xticklabels(xlabels, fontsize= ) plt.set(ax.get_xticklabels(), fontsize=) ax.tick_params(axis='x', labelsize= ) Set italic import matplotlib.pyplot as plt plt.title('$\\it{text you want to show in italics}$') plt.show() Set subscript import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.set(title=r'This is an expression $\\mathregular{e^{\\sin(\\omega\\phi)}}$', xlabel='meters $\\mathregular{10^1}$', # superscript ylabel=r'Hertz $\\mathregular{(\\frac{1}{s})}$') plt.show() Set y axis log plt.yscale(\"log\",base=2) Set axis range ax.set_xlim([25,50]) Hide axis ticks and tick labels plt.xticks([]) plt.yticks([]) Hide axis ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_linewidth(0.5) ax.spines['left'].set_linewidth(0.5) Set the separation between tick labels and axis labels ax.set_xlabel(labelpad=-0.3) Set the spearation between ticks and tick labels plt.tick_params(axis='both', pad =-0.3) Set ticks plt.yticks([-3,-2,-1,0,1,2,3], size=9) # the following code set the xtick interval to 10 from matplotlib.ticker import MultipleLocator majorLocator = MultipleLocator(10) ax.xaxis.set_major_locator(majorLocator) # or alternatively ax.xaxis.set_major_locator(MultipleLocator(10)) Set axes positions plt.axes(position=[0.26,0.21, 0.70, 0.75]) Adjust spacing between subplots fig, ax = plt.subplots(2, 2) fig.tight_layout(h_pad=2) # importing packages import numpy as np import matplotlib.pyplot as plt    # create data x=np.array([1, 2, 3, 4, 5])    # making subplots fig, ax = plt.subplots(2, 2)    # set data with subplots and plot ax[0, 0].plot(x, x) ax[0, 1].plot(x, x*2) ax[1, 0].plot(x, x*x) ax[1, 1].plot(x, x*x*x)    # set the spacing between subplots plt.subplots_adjust(left=0.1,                     bottom=0.1,                      right=0.9,                      top=0.9,                      wspace=0.4,                      hspace=0.4) plt.show() Use math format tick labels ax.ticklabel_format(axis='y', style='sci', scilimits=[-4,4], useMathText=True) Vertical lines # Thick red horizontal line at y=0 that spans the xrange. ax.axhline(linewidth=8, color='#d62728') # Horizontal line at y=1 that spans the xrange. ax.axhline(y=1) # Vertical line at x=1 that spans the yrange. ax.axvline(x=1) # Thick blue vertical line at x=0 that spans the upper quadrant of the yrange. ax.axvline(x=0, ymin=0.75, linewidth=8, color='#1f77b4') # Default hline at y=.5 that spans the middle half of the axes. ax.axhline(y=.5, xmin=0.25, xmax=0.75) # Infinite black line going through (0, 0) to (1, 1). ax.axline((0, 0), (1, 1), color='k') # 50%-gray rectangle spanning the axes' width from y=0.25 to y=0.75. ax.axhspan(0.25, 0.75, facecolor='0.5') # Green rectangle spanning the axes' height from x=1.25 to x=1.55. ax.axvspan(1.25, 1.55, facecolor='#2ca02c') Create colourmaps Since the methods used in other answers seems quite complicated for such easy task, here is a new answer: Instead of a ListedColormap, which produces a discrete colormap, you may use a LinearSegmentedColormap. This can easily be created from a list using the from_list method. import numpy as np import matplotlib.pyplot as plt import matplotlib.colors x,y,c = zip(*np.random.rand(30,3)*4-2) norm=plt.Normalize(-2,2) cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"violet\",\"blue\"]) plt.scatter(x,y,c=c, cmap=cmap, norm=norm) plt.colorbar() plt.show() More generally, if you have a list of values (e.g. [-2., -1, 2]) and corresponding colors, (e.g. [\"red\",\"violet\",\"blue\"]), such that the nth value should correspond to the nth color, you can normalize the values and supply them as tuples to the from_list method. import numpy as np import matplotlib.pyplot as plt import matplotlib.colors x,y,c = zip(*np.random.rand(30,3)*4-2) cvals = [-2., -1, 2] colors = [\"red\",\"violet\",\"blue\"] norm=plt.Normalize(min(cvals),max(cvals)) tuples = list(zip(map(norm,cvals), colors)) cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", tuples) plt.scatter(x,y,c=c, cmap=cmap, norm=norm) plt.colorbar() plt.show()"
  },"/Materials-Research-Notes/Chap6-Visualisation/pandas_tricks/": {
    "title": "Pandas tricks",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap6-Visualisation/pandas_tricks/",
    "body": "Dataframes Merge dataframes #on: the key (common column names) of the two dataframes; #how: inner (keep common values only), outer: keep all values df = pd.merge(df1, df2, how=\"inner\",on=\"wavelength\") Concatenate pandas.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=None, copy=True) Add new row to dataframe Just assign row to a particular index, using loc: df.loc[-1] = [2, 3, 4] # adding a row df.index = df.index + 1 # shifting index df = df.sort_index() # sorting by index **#add row to end of DataFrame df.loc[len(df.index)] = [value1, value2, value3, ...]** Add a column to pandas dataframe You can use the assign() function to add a new column to the end of a pandas DataFrame: df = df.assig…… You can use the assign() function to add a new column to the end of a pandas DataFrame: df = df.assign(col_name=[value1, value2, value3, ...]) And you can use the insert() function to add a new column to a specific location in a pandas DataFrame: df.insert(position, 'col_name', [value1, value2, value3, ...]) The following examples show how to use this syntax in practice with the following pandas DataFrame: import pandas as pd #create DataFrame df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23], 'assists': [5, 7, 7, 9, 12, 9], 'rebounds': [11, 8, 10, 6, 6, 5]}) #view DataFrame df points assists rebounds 0 25 5 11 1 12 7 8 2 15 7 10 3 14 9 6 4 19 12 6 5 23 9 5 Example 1: Add New Column to End of DataFrame The following code shows how to add a new column to the end of the DataFrame: #add 'steals' column to end of DataFrame df = df.assign(steals=[2, 2, 4, 7, 4, 1]) #view DataFrame df points assists rebounds steals 0 25 5 11 2 1 12 7 8 2 2 15 7 10 4 3 14 9 6 7 4 19 12 6 4 5 23 9 5 1 Example 2: Add Multiple Columns to End of DataFrame The following code shows how to add multiple new columns to the end of the DataFrame: #add 'steals' and 'blocks' columns to end of DataFrame df = df.assign(steals=[2, 2, 4, 7, 4, 1], blocks=[0, 1, 1, 3, 2, 5]) #view DataFrame df points assists rebounds steals blocks 0 25 5 11 2 0 1 12 7 8 2 1 2 15 7 10 4 1 3 14 9 6 7 3 4 19 12 6 4 2 5 23 9 5 1 5 Example 3: Add New Column Using Existing Column The following code shows how to add a new column to the end of the DataFrame, based on the values in an existing column: #add 'half_pts' to end of DataFrame df = df.assign(half_pts=lambda x: x.points / 2) #view DataFrame df points assists rebounds half_pts 0 25 5 11 12.5 1 12 7 8 6.0 2 15 7 10 7.5 3 14 9 6 7.0 4 19 12 6 9.5 5 23 9 5 11.5 Example 4: Add New Column in Specific Location of DataFrame The following code shows how to add a new column by inserting it into a specific location in the DataFrame: #add 'steals' to column index position 2 in DataFrame df.insert(2, 'steals', [2, 2, 4, 7, 4, 1]) #view DataFrame df points assists steals rebounds 0 25 5 2 11 1 12 7 2 8 2 15 7 4 10 3 14 9 7 6 4 19 12 4 6 5 23 9 1 5 Data Replace values # Replace a Single Value df['Age'] = df['Age'].replace(23, 99) # Replace Multiple Values df['Age'] = df['Age'].replace([23, 45], [99, 999]) # Also works in the Entire DataFrame df = df.replace(23, 99) df = df.replace([23, 45], [99, 999]) # Replace Multiple Values with a Single Value df['Age'] = df['Age'].replace([23, 45, 35], 99) # Using a Dictionary (Dict is passed into to_replace=) df['Age'] = df['Age'].replace({23:99, 45:999}) # Using a Dictionary for Column Replacements (key:value = column:value) df = df.replace({'Name': 'Jane', 'Age': 45}, 99) Index, slicing, filtering &amp; sorting Reset index pandas.DataFrame.reset_index — pandas 1.2.4 documentation (pydata.org) df.reset_index(drop=True) Set first column as index df = pd.read_excel(file, header=0, index_col=0) Select specified columns: df.filter([\"species\",\"bill_length_mm\"]) Filtering a column by multiple conditions: dataFrame.loc[(dataFrame['Salary']&gt;=100000) &amp; (dataFrame['Age']&lt; 40) &amp; (dataFrame['JOB'].str.startswith('D')), ['Name','JOB']] # Select the data by condition: 449.75&lt;data['e loss']&lt;470.25 data[(449.75&lt;data['e loss']) &amp; (data['e loss']&lt;470.25)] # Select the data by condition: data['e loss'] &lt; 449.75or data['e loss'] &gt; 470.25 data[(449.75&lt;data['e loss']) | (data['e loss']&lt;470.25)] Slicing #select the values after row 209 df=df1.iloc[209:] #select df=df1.iloc[156:190] Sorting df_sort = df.sort_values(by = 'column_1') df_sort = df.sort_values(by = ['column_1', 'column_2']) File I/O Read txt import pandas as pd data = pd.read_csv('output_list.txt', header = None) Skip rows pandas.read_csv(filepath_or_buffer, skiprows=N, ....) If the unwanted rows are begin with #, than these rows can be ignored by: df=pd.read_csv(\"1-Signal-43135.msa\", header=None, sep = ',', comment='#', delimiter=',',dtype=float) Specify column names Use names to specify the column names: df = pd.read_csv(path, header=None, sep = ',', comment='#',                  delimiter=',', names=('e loss', 'intensity'),                  dtype=float) Write data to excel file Create, write to and save a workbook: df1 = pd.DataFrame([['a', 'b'], ['c', 'd']], index=['row 1', 'row 2'], columns=['col 1', 'col 2']) df1.to_excel(\"output.xlsx\") To specify the sheet name: df1.to_excel(\"output.xlsx\",              sheet_name='Sheet_name_1') If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: df2 = df1.copy() with pd.ExcelWriter('output.xlsx') as writer:     df1.to_excel(writer, sheet_name='Sheet_name_1')     df2.to_excel(writer, sheet_name='Sheet_name_2') ExcelWriter can also be used to append to an existing Excel file: with pd.ExcelWriter('output.xlsx', engine=\"openpyxl\", mode='a') as writer:     df.to_excel(writer, sheet_name='Sheet_name_3') To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension): df1.to_excel('output1.xlsx', engine='xlsxwriter')"
  },"/Materials-Research-Notes/Chap7-sys_ssh/work_on_ssh_session/": {
    "title": "Bash, Git &amp; SSH tricks",
    "keywords": "notes",
    "url": "/Materials-Research-Notes/Chap7-sys_ssh/work_on_ssh_session/",
    "body": "SSH Set up List of Linux HPCs and workstations Start session Specify port If you would like to specify the port so that you can tunneling to this session (e.g. when using jupyter notebook): ssh -L 8888:127.0.0.1:8888 username@hostname Files Download files from server scp -r username@hostname:path_of_file local_path Copy file(s) to multiple directory $ echo dir1 dir2 dir3 | xargs -n 1 cp -v myfile 'myfile' -&gt; 'dir1/myfile' 'myfile' -&gt; 'dir2/myfile' 'myfile' -&gt; 'dir3/myfile' Find files Find files modified within the last 7 days find . -type f -mtime -7 -print Delete files found and print the file name find . -name slurm*.out -delete -print Print tail of multiple files tail ./*/vasp.log Print contents of file Specified lines of a file If you want lines X to Y inclusive (starting the numbering at 1), use tail -n \"+$X\" /path/to/file | head -n \"$((Y-X+1))\" Print entire file Using cat cat file.txt Using echo echo \"$(&lt;file.txt )\" Slurm Cancel jobs scancel --state=PENDING --user=leilei"
  },"/Materials-Research-Notes/ChapX-Info/about/": {
    "title": "About",
    "keywords": "pages",
    "url": "/Materials-Research-Notes/ChapX-Info/about/",
    "body": "I am Lei Lei, PhD in the Materials Science. I am currently working in the Faculty of Engineering, University of Nottingham as a Research Fellow, and this is my private Notebook on machine learning, materials science, data visualisation, python coding and more. Please contact me if you have any questions or feedback/suggestions on this project. Email: Lei.Lei2@nottingham.ac.uk. Follow me on: Twiter Facebook"
  },"/Materials-Research-Notes/ChapX-Info/quick-start/": {
    "title": "Quick Start",
    "keywords": "pages",
    "url": "/Materials-Research-Notes/ChapX-Info/quick-start/",
    "body": "This notebook is created by using the gitbook theme created using jekyll. Some files are modified and the style is slightly changed from the original version. The notes are written in the kramdown variant of markdown. Markdown code block. touch init.py Highlighted using Liquid template language. touch init.py"
  }}
